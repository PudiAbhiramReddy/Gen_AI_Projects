# -*- coding: utf-8 -*-
"""Fully integrated code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rmq2u3oVvM7Nd6n35yfnenxsLOieJj8f
"""

#habibi

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torch.utils.data import DataLoader, Dataset
from torchvision.transforms import InterpolationMode
import pandas as pd
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import os
import time
import random

# --- 0. Device Configuration ---
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# --- 1. Hyperparameters (Combined and Adjusted) ---
# Common Hyperparameters
IMAGE_FOLDER_PATH = "/home/fu/travis/Projects/Metasurfaces/AI-Guided-Metasurfaces-for-Controlling-Thermal-Radiation/Data/Data_Generated_Images/a_circles"
TRAIN_METADATA_FILE = "AI-Guided-Metasurfaces-for-Controlling-Thermal-Radiation/metasurface_absorbance_updated.csv"
TEST_METADATA_FILE = "/home/fu/travis/Projects/Metasurfaces/AI-Guided-Metasurfaces-for-Controlling-Thermal-Radiation/metasurface_absorbance_test_updated.csv" # Still used for sim evaluation

IMAGE_SIZE = 64
CHANNELS = 1 # Grayscale images
NUM_ANGLES = 15 # Output dimensions for absorbance spectrum

# GAN Hyperparameters
LATENT_DIM = 100 # Size of the latent z vector
GF = 64 # Generator feature map depth
DF = 64 # Discriminator feature map depth

# Simulator Hyperparameters (from previous code, ensures consistency)
NDF = 64 # Discriminator/Simulator feature map depth for CNN

# Training Hyperparameters
BATCH_SIZE = 8 # Keep consistent for both
LEARNING_RATE_G = 0.0002 # Generator learning rate
LEARNING_RATE_D = 0.0002 # Discriminator learning rate
LEARNING_RATE_SIM = 0.0002 # Simulator learning rate

NUM_EPOCHS_SIMULATOR = 100 # Epochs for pre-training the simulator
NUM_EPOCHS_GAN = 100 # Epochs for GAN training

WORKERS = 2

# Loss weighting parameter for Generator (from the paper)
LAMBDA_SIM_LOSS = 0.0001 # λ from the paper, balancing simulator and discriminator loss for G

OUTPUT_DIR_BASE = "gan_simulator_combined_output" # Base output directory
OUTPUT_DIR_SIM = os.path.join(OUTPUT_DIR_BASE, "simulator_output")
OUTPUT_DIR_GAN = os.path.join(OUTPUT_DIR_BASE, "gan_output")

os.makedirs(OUTPUT_DIR_SIM, exist_ok=True)
os.makedirs(OUTPUT_DIR_GAN, exist_ok=True)

# --- Weight Initializers (DCGAN-style) ---
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
        if m.bias is not None:
            nn.init.constant_(m.bias.data, 0)
    elif classname.find('BatchNorm2d') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)
    elif classname.find('Linear') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
        if m.bias is not None:
            nn.init.constant_(m.bias.data, 0)

# --- 2. Custom Dataset (Shared for both Simulator and GAN) ---
class SingleFolderMetasurfaceDataset(Dataset):
    def __init__(self, metadata_file, image_folder_path, transform=None):
        try:
            self.metadata_df = pd.read_csv(metadata_file)
        except FileNotFoundError:
            print(f"Error: Metadata file not found at {metadata_file}")
            raise
        self.image_folder_path = image_folder_path
        self.transform = transform
        if len(self.metadata_df.columns) < NUM_ANGLES + 1:
            raise ValueError(f"CSV file must have at least {NUM_ANGLES + 1} columns (filename + {NUM_ANGLES} absorbances)")
        self.absorbance_cols = self.metadata_df.columns[1:NUM_ANGLES+1].tolist()

        if 'Image Name' not in self.metadata_df.columns and self.metadata_df.columns[0].lower() != 'image name':
            if 'filename' in self.metadata_df.columns:
                self.filename_col = 'filename'
            else:
                print("Warning: 'Image Name' column not found. Assuming first column is filename.")
                self.filename_col = self.metadata_df.columns[0]
        else:
            self.filename_col = 'Image Name'

    def __len__(self):
        return len(self.metadata_df)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        img_filename_in_csv = self.metadata_df.iloc[idx][self.filename_col]
        img_full_path = os.path.join(self.image_folder_path, img_filename_in_csv)
        try:
            image = Image.open(img_full_path).convert('L')
        except FileNotFoundError:
            print(f"Error: Image file not found at {img_full_path} (referenced in {self.metadata_df.iloc[idx][self.filename_col]})")
            raise FileNotFoundError(f"Image file not found: {img_full_path}")
        absorbance_vector = self.metadata_df.iloc[idx][self.absorbance_cols].values.astype(np.float32)
        absorbance_tensor = torch.from_numpy(absorbance_vector)
        if self.transform:
            image = self.transform(image)
        return image, absorbance_tensor

# Define common image transformations
image_transforms = transforms.Compose([
    transforms.Resize(IMAGE_SIZE, interpolation=InterpolationMode.BICUBIC),
    transforms.CenterCrop(IMAGE_SIZE),
    transforms.ToTensor(),
    transforms.Normalize((0.5,) * CHANNELS, (0.5,) * CHANNELS)
])

# Setup Datasets and DataLoaders
try:
    train_dataset = SingleFolderMetasurfaceDataset(metadata_file=TRAIN_METADATA_FILE,
                                                       image_folder_path=IMAGE_FOLDER_PATH,
                                                       transform=image_transforms)
    if len(train_dataset) == 0: raise ValueError("Training dataset is empty.")
    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS,
                                    drop_last=True if len(train_dataset) > BATCH_SIZE else False)
    print(f"Total training images: {len(train_dataset)}")

    test_dataloader = None
    if os.path.exists(TEST_METADATA_FILE):
        test_dataset = SingleFolderMetasurfaceDataset(metadata_file=TEST_METADATA_FILE,
                                                          image_folder_path=IMAGE_FOLDER_PATH,
                                                          transform=image_transforms)
        if len(test_dataset) > 0:
            test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS, drop_last=False)
            print(f"Total test images: {len(test_dataset)}")
        else:
            print("Warning: Test CSV loaded but dataset is empty.")
            test_dataloader = None
    else:
        print(f"Warning: Test metadata file '{TEST_METADATA_FILE}' not found.")

    if len(train_dataset) > 0:
        sample_batch_img, sample_batch_abs = next(iter(train_dataloader))
        plt.figure(figsize=(8,8))
        plt.axis("off")
        plt.title(f"Sample Training Images")
        plt.imshow(np.transpose(vutils.make_grid(sample_batch_img.to(device)[:min(16, BATCH_SIZE)], padding=2, normalize=True).cpu(),(1,2,0)))
        plt.savefig(os.path.join(OUTPUT_DIR_BASE, "sample_training_images_pytorch.png"))
        plt.show()
        print(f"Sample absorbance vector (first in training batch, shape: {sample_batch_abs[0].shape}):\n{sample_batch_abs[0]}")

except Exception as e:
    print(f"Error setting up dataset/dataloader: {e}")
    raise e

# --- 3. Model Definitions ---

# Generator (G)
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.main = nn.Sequential(
            # Input is Z, going into a convolution
            nn.ConvTranspose2d(LATENT_DIM, GF * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(GF * 8),
            nn.ReLU(True),
            # State size: (GF*8) x 4 x 4
            nn.ConvTranspose2d(GF * 8, GF * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(GF * 4),
            nn.ReLU(True),
            # State size: (GF*4) x 8 x 8
            nn.ConvTranspose2d(GF * 4, GF * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(GF * 2),
            nn.ReLU(True),
            # State size: (GF*2) x 16 x 16
            nn.ConvTranspose2d(GF * 2, GF, 4, 2, 1, bias=False),
            nn.BatchNorm2d(GF),
            nn.ReLU(True),
            # State size: (GF) x 32 x 32
            nn.ConvTranspose2d(GF, CHANNELS, 4, 2, 1, bias=False),
            nn.Tanh() # Output images are normalized to [-1, 1]
            # State size: (CHANNELS) x 64 x 64
        )

    def forward(self, input):
        return self.main(input)

# Discriminator (D)
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            # Input is (CHANNELS) x 64 x 64
            nn.Conv2d(CHANNELS, DF, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # State size: (DF) x 32 x 32
            nn.Conv2d(DF, DF * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(DF * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # State size: (DF*2) x 16 x 16
            nn.Conv2d(DF * 2, DF * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(DF * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # State size: (DF*4) x 8 x 8
            nn.Conv2d(DF * 4, DF * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(DF * 8),
            nn.LeakyReLU(0.2, inplace=True),
            # State size: (DF*8) x 4 x 4
            nn.Conv2d(DF * 8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid() # Output is probability between 0 and 1
        )

    def forward(self, input):
        return self.main(input)

# Simulator (S)
class RobustSimulatorCNN(nn.Module):
    def __init__(self, num_outputs=NUM_ANGLES, ndf=NDF):
        super(RobustSimulatorCNN, self).__init__()
        self.cnn_layers = nn.Sequential(
            nn.Conv2d(CHANNELS, ndf, kernel_size=4, stride=2, padding=1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ndf * 2),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ndf * 4, ndf * 8, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ndf * 8, ndf * 8, kernel_size=4, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(ndf*8),
            nn.LeakyReLU(0.2, inplace=True)
        )
        with torch.no_grad():
            dummy_input = torch.randn(1, CHANNELS, IMAGE_SIZE, IMAGE_SIZE)
            self.cnn_layers.eval() # Ensure eval mode for correct BatchNorm behavior during dummy pass
            cnn_out_size = self.cnn_layers(dummy_input).view(1, -1).size(1)
            self.cnn_layers.train() # Set back to train mode if needed elsewhere in init, or let model.train() handle it

        self.fc_layers = nn.Sequential(
            nn.Linear(cnn_out_size, 512),
            nn.ReLU(True),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(True),
            nn.Dropout(0.5),
            nn.Linear(256, num_outputs),
            nn.Sigmoid() # Output absorbance values between 0 and 1
        )
    def forward(self, image_input):
        cnn_out = self.cnn_layers(image_input)
        flattened = cnn_out.view(cnn_out.size(0), -1)
        output = self.fc_layers(flattened)
        return output

# Instantiate models
netG = Generator().to(device)
netD = Discriminator().to(device)
simulator_model = RobustSimulatorCNN(num_outputs=NUM_ANGLES, ndf=NDF).to(device)

# Apply weights_init
netG.apply(weights_init)
netD.apply(weights_init)
simulator_model.apply(weights_init)

# --- 4. Loss Functions and Optimizers ---
# For Discriminator/Generator
criterion_D = nn.BCELoss() # Binary Cross-Entropy for GAN loss
# For Simulator
criterion_S = nn.MSELoss() # MSE for simulator loss

# Setup Adam optimizers
optimizerD = optim.Adam(netD.parameters(), lr=LEARNING_RATE_D, betas=(0.5, 0.999))
optimizerG = optim.Adam(netG.parameters(), lr=LEARNING_RATE_G, betas=(0.5, 0.999))
optimizer_sim = optim.Adam(simulator_model.parameters(), lr=LEARNING_RATE_SIM, betas=(0.5, 0.999))


# Labels for GAN training
real_label = 1.
fake_label = 0.

# Fixed noise for Generator visualization
fixed_noise = torch.randn(min(64, BATCH_SIZE), LATENT_DIM, 1, 1, device=device)

# --- 5. Pre-train Simulator ---
print("--- Starting Simulator Pre-training ---")
train_losses_sim = []
test_losses_sim = []

for epoch in range(NUM_EPOCHS_SIMULATOR):
    simulator_model.train()
    running_train_loss = 0.0
    epoch_start_time = time.time()

    for i, (images, targets) in enumerate(train_dataloader):
        images = images.to(device)
        targets = targets.to(device)

        optimizer_sim.zero_grad()
        outputs = simulator_model(images)
        loss = criterion_S(outputs, targets)
        loss.backward()
        optimizer_sim.step()
        running_train_loss += loss.item() * images.size(0)

    epoch_train_loss = running_train_loss / len(train_dataset)
    train_losses_sim.append(epoch_train_loss)

    epoch_test_loss_val = float('nan')
    if test_dataloader:
        simulator_model.eval()
        running_test_loss = 0.0
        with torch.no_grad():
            for images_test, targets_test in test_dataloader:
                images_test = images_test.to(device)
                targets_test = targets_test.to(device)
                outputs_test = simulator_model(images_test)
                loss_test = criterion_S(outputs_test, targets_test)
                running_test_loss += loss_test.item() * images_test.size(0)
        epoch_test_loss_val = running_test_loss / len(test_dataset)
        test_losses_sim.append(epoch_test_loss_val)
        print(f"Simulator Epoch [{epoch+1}/{NUM_EPOCHS_SIMULATOR}], Train Loss: {epoch_train_loss:.6f}, Test Loss: {epoch_test_loss_val:.6f}, Time: {time.time()-epoch_start_time:.2f}s")
    else:
        print(f"Simulator Epoch [{epoch+1}/{NUM_EPOCHS_SIMULATOR}], Train Loss: {epoch_train_loss:.6f}, Time: {time.time()-epoch_start_time:.2f}s")

print("--- Simulator Pre-training Finished. ---")

# Plotting Simulator Training and Test Loss
plt.figure(figsize=(10,5))
plt.title("Simulator Training & Test Loss")
plt.plot(train_losses_sim, label="Train MSE Loss")
if test_losses_sim:
    plt.plot(test_losses_sim, label="Test MSE Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.savefig(os.path.join(OUTPUT_DIR_SIM, "simulator_loss_curve_pytorch.png"))
plt.show()

# Set simulator to evaluation mode and freeze its parameters
simulator_model.eval()
for param in simulator_model.parameters():
    param.requires_grad = False
print("Simulator model frozen for GAN training.")


# --- 6. Combined GAN Training Loop ---
print("\n--- Starting Combined GAN Training (Generator, Discriminator, Simulator) ---")

# Lists to keep track of progress
G_losses = []
D_losses = []
Sim_losses_G_side = [] # Simulator loss when predicting on generator's output
iters = 0

print("Starting Training Loop...")
# For each epoch
for epoch in range(NUM_EPOCHS_GAN):
    for i, (real_images, target_absorbances) in enumerate(train_dataloader):
        ############################
        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))
        ###########################
        ## Train with all-real batch
        netD.zero_grad()
        # Format batch
        real_cpu = real_images.to(device)
        # --- ADD THIS LINE ---
        target_absorbances = target_absorbances.to(device) # Ensure target_absorbances is on the GPU
        # ---------------------

        b_size = real_cpu.size(0)
        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)
        # Forward pass real batch through D
        output = netD(real_cpu).view(-1)
        # Calculate loss on all-real batch
        errD_real = criterion_D(output, label)
        # Calculate gradients for D in backward pass
        errD_real.backward()
        D_x = output.mean().item()

        ## Train with all-fake batch
        # Generate fake image batch with G
        noise = torch.randn(b_size, LATENT_DIM, 1, 1, device=device)
        fake = netG(noise)
        label.fill_(fake_label)
        # Classify all fake batch with D
        output = netD(fake.detach()).view(-1) # .detach() prevents gradients from flowing back to G
        # Calculate D's loss on the all-fake batch
        errD_fake = criterion_D(output, label)
        # Calculate the gradients for this batch, accumulated (summed) with previous gradients
        errD_fake.backward()
        D_G_z1 = output.mean().item()
        # Compute total error for D
        errD = errD_real + errD_fake
        # Update D
        optimizerD.step()

        ############################
        # (2) Update G network: maximize log(D(G(z))) + λ * MSE(S(G(z)), target_absorbances)
        ###########################
        netG.zero_grad()
        label.fill_(real_label)  # fake labels are real for generator cost

        # Generator forward pass with new noise
        noise = torch.randn(b_size, LATENT_DIM, 1, 1, device=device)
        fake = netG(noise)

        # 1. Discriminator Loss for Generator (L_D from paper)
        output = netD(fake).view(-1)
        errG_D = criterion_D(output, label) # G wants D to classify fakes as real

        # 2. Simulator Loss for Generator (L_S from paper)
        simulator_output = simulator_model(fake) # S(G(z))
        errG_S = criterion_S(simulator_output, target_absorbances) # MSE loss with real targets

        # Combined Generator Loss (L_G = L_S + λ L_D)
        errG = errG_D + LAMBDA_SIM_LOSS * errG_S
        errG.backward()
        D_G_z2 = output.mean().item()
        optimizerG.step()

        # Output training stats
        if i % 50 == 0:
            print(f'[{epoch+1}/{NUM_EPOCHS_GAN}][{i}/{len(train_dataloader)}] '
                  f'Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f} '
                  f'D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f} / {D_G_z2:.4f} '
                  f'Sim_Loss_G_side: {errG_S.item():.4f}')

        # Save Losses for plotting later
        G_losses.append(errG.item())
        D_losses.append(errD.item())
        Sim_losses_G_side.append(errG_S.item()) # This loss is for generated images

        iters += 1

    # Checkpointing and visualization after each epoch
    with torch.no_grad():
        fake_fixed_noise = netG(fixed_noise).detach().cpu()
        vutils.save_image(fake_fixed_noise,
                          f'{OUTPUT_DIR_GAN}/fake_samples_epoch_{epoch+1:03d}.png',
                          normalize=True)

    # Save model checkpoints
    if (epoch + 1) % 10 == 0 or (epoch + 1) == NUM_EPOCHS_GAN:
        torch.save(netG.state_dict(), f'{OUTPUT_DIR_GAN}/netG_epoch_{epoch+1}.pth')
        torch.save(netD.state_dict(), f'{OUTPUT_DIR_GAN}/netD_epoch_{epoch+1}.pth')
        print(f"Saved models at epoch {epoch+1}")


print("--- Combined GAN Training Finished. ---")

# Plotting GAN training losses
plt.figure(figsize=(10,5))
plt.title("Generator and Discriminator Loss During GAN Training")
plt.plot(G_losses, label="G Loss (Combined)")
plt.plot(D_losses, label="D Loss")
plt.plot(Sim_losses_G_side, label="Simulator Loss on G Images (for G)")
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.legend()
plt.savefig(os.path.join(OUTPUT_DIR_GAN, "gan_combined_loss_curve.png"))
plt.show()

# --- Final Evaluation on Generator Quality and Simulator Performance on Generated Images ---

print("\n--- Final Evaluation of Generator and Simulator on Generated Images ---")
netG.eval()
simulator_model.eval() # Ensure simulator is still in eval mode

# Generate a larger batch of fake images for final evaluation
num_eval_images = 100
eval_noise = torch.randn(num_eval_images, LATENT_DIM, 1, 1, device=device)
with torch.no_grad():
    generated_images = netG(eval_noise).detach()
    predicted_absorbances = simulator_model(generated_images).cpu().numpy()

# Save some generated images
vutils.save_image(generated_images[:min(64, num_eval_images)],
                   f'{OUTPUT_DIR_GAN}/final_generated_samples.png',
                   normalize=True)

# Analyze predicted absorbances from generated images
print(f"\nSample Predicted Absorbances from Generated Images (first {min(5, num_eval_images)}):\n")
for i in range(min(5, num_eval_images)):
    print(f"Sample {i+1}: {[f'{x:.3f}' for x in predicted_absorbances[i]]}")

# To get a meaningful evaluation of the *simulator's performance*, you'd still compare
# its predictions on *real* test data against the ground truth.
# The `avg_final_test_mse_loss` and `avg_final_test_mae` calculated after simulator pre-training
# are the true measures of simulator accuracy.
# The `Sim_losses_G_side` in the GAN training plot tells you how well the generator
# is producing images that, when simulated, yield properties similar to *real* training data properties.

print(f"\nAll outputs saved in: {OUTPUT_DIR_BASE}")