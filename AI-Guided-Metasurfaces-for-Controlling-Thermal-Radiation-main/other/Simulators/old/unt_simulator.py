# -*- coding: utf-8 -*-
"""UNT_Simulator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KCxjqcTDb9cHoDFau24NGaTvjVpdz01S
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from torchvision.transforms import InterpolationMode
import torchvision.utils as vutils
from PIL import Image
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import time

from google.colab import drive
drive.mount('/content/drive')

# --- 0. Setup and Device Configuration ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

#zip

import zipfile
import os

# Path to zip file
zip_path = '/content/drive/MyDrive/Data (1).zip'

# Destination folder to unzip
extract_to = '/content/drive/My Drive/MetaSurfaces_Final2/'

# Unzipping
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_to)

print("Unzipped successfully to:", extract_to)

# --- 1. Hyperparameters ---
IMAGE_FOLDER_PATH = "/content/drive/MyDrive/MetaSurfaces_Final2/Data/a_circles"
TRAIN_METADATA_FILE = "/content/metasurface_absorbance_updated.csv"
TEST_METADATA_FILE = "/content/metasurface_absorbance_test_updated.csv"

IMAGE_SIZE = 64
CHANNELS = 1
NUM_ANGLES = 15

BATCH_SIZE = 8
LEARNING_RATE = 0.0002
NUM_EPOCHS = 100 # Adjust as needed
WORKERS = 2
NDF = 64

OUTPUT_DIR_SIM = "simulator_output" # New output dir name
os.makedirs(OUTPUT_DIR_SIM, exist_ok=True) # Still create for plots

# --- Weight Initializers (DCGAN-style) ---
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
        if m.bias is not None:
            nn.init.constant_(m.bias.data, 0)
    elif classname.find('BatchNorm2d') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)
    elif classname.find('Linear') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
        if m.bias is not None:
            nn.init.constant_(m.bias.data, 0)

# --- 2. Custom Dataset ---
class SingleFolderMetasurfaceDataset(Dataset):
    def __init__(self, metadata_file, image_folder_path, transform=None):
        try:
            self.metadata_df = pd.read_csv(metadata_file)
        except FileNotFoundError:
            print(f"Error: Metadata file not found at {metadata_file}")
            raise
        self.image_folder_path = image_folder_path
        self.transform = transform
        if len(self.metadata_df.columns) < NUM_ANGLES + 1:
            raise ValueError(f"CSV file must have at least {NUM_ANGLES + 1} columns (filename + {NUM_ANGLES} absorbances)")
        self.absorbance_cols = self.metadata_df.columns[1:NUM_ANGLES+1].tolist()

        if 'Image Name' not in self.metadata_df.columns and self.metadata_df.columns[0].lower() != 'image name':
            if 'filename' in self.metadata_df.columns:
                self.filename_col = 'filename'
            else:
                print("Warning: 'Image Name' column not found. Assuming first column is filename.")
                self.filename_col = self.metadata_df.columns[0]
        else:
            self.filename_col = 'Image Name'

    def __len__(self):
        return len(self.metadata_df)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        img_filename_in_csv = self.metadata_df.iloc[idx][self.filename_col]
        img_full_path = os.path.join(self.image_folder_path, img_filename_in_csv)
        try:
            image = Image.open(img_full_path).convert('L')
        except FileNotFoundError:
            print(f"Error: Image file not found at {img_full_path} (referenced in {self.metadata_df.iloc[idx][self.filename_col]})")
            raise FileNotFoundError(f"Image file not found: {img_full_path}")
        absorbance_vector = self.metadata_df.iloc[idx][self.absorbance_cols].values.astype(np.float32)
        absorbance_tensor = torch.from_numpy(absorbance_vector)
        if self.transform:
            image = self.transform(image)
        return image, absorbance_tensor

image_transforms = transforms.Compose([
    transforms.Resize(IMAGE_SIZE, interpolation=InterpolationMode.BICUBIC),
    transforms.CenterCrop(IMAGE_SIZE),
    transforms.ToTensor(),
    transforms.Normalize((0.5,) * CHANNELS, (0.5,) * CHANNELS)
])

try:
    train_dataset_sim = SingleFolderMetasurfaceDataset(metadata_file=TRAIN_METADATA_FILE,
                                                       image_folder_path=IMAGE_FOLDER_PATH,
                                                       transform=image_transforms)
    if len(train_dataset_sim) == 0: raise ValueError("Training dataset is empty.")
    train_dataloader_sim = DataLoader(train_dataset_sim, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS,
                                    drop_last=True if len(train_dataset_sim) > BATCH_SIZE else False)
    print(f"Total training images: {len(train_dataset_sim)}")

    test_dataloader_sim = None
    if os.path.exists(TEST_METADATA_FILE):
        test_dataset_sim = SingleFolderMetasurfaceDataset(metadata_file=TEST_METADATA_FILE,
                                                          image_folder_path=IMAGE_FOLDER_PATH,
                                                          transform=image_transforms)
        if len(test_dataset_sim) > 0:
            test_dataloader_sim = DataLoader(test_dataset_sim, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS, drop_last=False)
            print(f"Total test images: {len(test_dataset_sim)}")
        else:
            print("Warning: Test CSV loaded but dataset is empty.")
            test_dataloader_sim = None
    else:
        print(f"Warning: Test metadata file '{TEST_METADATA_FILE}' not found.")

    if len(train_dataset_sim) > 0:
        sample_batch_img, sample_batch_abs = next(iter(train_dataloader_sim))
        # ... (plotting sample images - code remains same)
        plt.figure(figsize=(8,8))
        plt.axis("off")
        plt.title(f"Sample Training Images for Simulator")
        plt.imshow(np.transpose(vutils.make_grid(sample_batch_img.to(device)[:min(16, BATCH_SIZE)], padding=2, normalize=True).cpu(),(1,2,0)))
        plt.savefig(os.path.join(OUTPUT_DIR_SIM, "sample_training_images_simulator_pytorch.png"))
        plt.show()
        print(f"Sample absorbance vector (first in training batch, shape: {sample_batch_abs[0].shape}):\n{sample_batch_abs[0]}")


except Exception as e:
    print(f"Error setting up dataset/dataloader: {e}")
    raise e


# --- 3. Simulator Model Definition ---
class RobustSimulatorCNN(nn.Module):
    def __init__(self, num_outputs=NUM_ANGLES, ndf=NDF):
        super(RobustSimulatorCNN, self).__init__()
        self.cnn_layers = nn.Sequential(
            nn.Conv2d(CHANNELS, ndf, kernel_size=4, stride=2, padding=1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ndf * 2),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ndf * 4, ndf * 8, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ndf * 8, ndf * 8, kernel_size=4, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(ndf*8),
            nn.LeakyReLU(0.2, inplace=True)
        )
        with torch.no_grad():
            dummy_input = torch.randn(1, CHANNELS, IMAGE_SIZE, IMAGE_SIZE)
            # Put the CNN layers in evaluation mode for the dummy pass
            self.cnn_layers.eval()
            cnn_out_size = self.cnn_layers(dummy_input).view(1, -1).size(1)
            # Return the CNN layers to training mode if needed for other parts of init,
            # though typically they are handled by model.train() later.
            # self.cnn_layers.train() # This line is not strictly necessary here
        #cnn_out_size = ndf * 8 # This line seems to override the calculated size, ensure this is intended.
                               # If the dummy input calculation was meant to be precise, remove this line.
                               # Based on the error, the dummy pass is failing, so we fix that first.
        self.fc_layers = nn.Sequential(
            nn.Linear(cnn_out_size, 512),
            nn.ReLU(True),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(True),
            nn.Dropout(0.5),
            nn.Linear(256, num_outputs),
            nn.Sigmoid()

        )
    def forward(self, image_input):
        cnn_out = self.cnn_layers(image_input)
        flattened = cnn_out.view(cnn_out.size(0), -1)
        output = self.fc_layers(flattened)
        return output

simulator_model = RobustSimulatorCNN(num_outputs=NUM_ANGLES, ndf=NDF).to(device)
simulator_model.apply(weights_init)
# print(simulator_model)
# total_params = sum(p.numel() for p in simulator_model.parameters() if p.requires_grad)
# print(f"Total trainable parameters in Simulator: {total_params}")

# --- 4. Loss Function and Optimizer ---
criterion_sim = nn.MSELoss()
optimizer_sim = optim.Adam(simulator_model.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))
# Using a simpler scheduler or none if not strictly needed for this version
#scheduler_sim = optim.lr_scheduler.StepLR(optimizer_sim, step_size=75, gamma=0.5)
# scheduler_sim = optim.lr_scheduler.ReduceLROnPlateau(optimizer_sim, mode='min', factor=0.5, patience=20, verbose=True)


# --- 5. Training Loop for Simulator (MODIFIED) ---
train_losses_sim = []
test_losses_sim = [] # Keep for plotting if test data exists
print("Starting Simulator Training...")

for epoch in range(NUM_EPOCHS):
    simulator_model.train()
    running_train_loss = 0.0
    epoch_start_time = time.time()

    for batch_idx, (images, targets) in enumerate(train_dataloader_sim):
        images = images.to(device)
        targets = targets.to(device)

        optimizer_sim.zero_grad()
        outputs = simulator_model(images)
        loss = criterion_sim(outputs, targets)
        loss.backward()
        optimizer_sim.step()
        running_train_loss += loss.item() * images.size(0)

    # Corrected: len(train_dataset_sim.dataset) if it's a Subset, else len(train_dataset_sim)
    # Since it's not a subset in this version, len(train_dataset_sim) is correct.
    epoch_train_loss = running_train_loss / len(train_dataset_sim)
    train_losses_sim.append(epoch_train_loss)

    # Evaluation on Test Set at the end of each epoch (if test_dataloader exists)
    epoch_test_loss_val = float('nan') # Use NaN if no test set
    if test_dataloader_sim:
        simulator_model.eval()
        running_test_loss = 0.0
        with torch.no_grad():
            for images_test, targets_test in test_dataloader_sim:
                images_test = images_test.to(device)
                targets_test = targets_test.to(device)
                outputs_test = simulator_model(images_test)
                loss_test = criterion_sim(outputs_test, targets_test)
                running_test_loss += loss_test.item() * images_test.size(0)
        # Corrected: len(test_dataset_sim.dataset) if it's a Subset, else len(test_dataset_sim)
        epoch_test_loss_val = running_test_loss / len(test_dataset_sim)
        test_losses_sim.append(epoch_test_loss_val)
        # MODIFIED: Removed LR from print as it's fixed now
        print(f"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {epoch_train_loss:.6f}, Test Loss: {epoch_test_loss_val:.6f}, Time: {time.time()-epoch_start_time:.2f}s")
    else:
        # MODIFIED: Removed LR from print as it's fixed now
        print(f"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {epoch_train_loss:.6f}, Time: {time.time()-epoch_start_time:.2f}s")

    # scheduler_sim.step() # REMOVED: No scheduler step needed

print("Simulator Training Finished.")


# --- Plotting Simulator Training and Test Loss ---
plt.figure(figsize=(10,5))
plt.title("Simulator Training & Test Loss")
plt.plot(train_losses_sim, label="Train MSE Loss")
if test_losses_sim: # Only plot if test_losses were recorded
    plt.plot(test_losses_sim, label="Test MSE Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.savefig(os.path.join(OUTPUT_DIR_SIM, "simulator_loss_curve_pytorch.png"))
plt.show()

# --- Final Evaluation on Test Set (using the model state at end of training) ---
if test_dataloader_sim:
    print("\nFinal Evaluation on Test Set (using model from end of training)...")
    simulator_model.eval()
    final_test_loss = 0.0
    all_targets_test = []
    all_predictions_test = []

    with torch.no_grad():
        for images_test, targets_test in test_dataloader_sim:
            images_test = images_test.to(device)
            targets_test = targets_test.to(device)
            outputs_test = simulator_model(images_test)
            loss_test = criterion_sim(outputs_test, targets_test)
            final_test_loss += loss_test.item() * images_test.size(0)
            all_targets_test.append(targets_test.cpu().numpy())
            all_predictions_test.append(outputs_test.cpu().numpy())

    avg_final_test_loss = final_test_loss / len(test_dataset_sim)
    print(f"Average Final Test Loss (MSE): {avg_final_test_loss:.6f}")

    all_targets_test = np.concatenate(all_targets_test, axis=0)
    all_predictions_test = np.concatenate(all_predictions_test, axis=0)

    print("\nSample Test Predictions (Target vs Predicted):")
    for i in range(min(5, len(all_targets_test))): # Iterate up to 5 or length of test set
        # Accessing filename from the original dataset object
        # Need to handle if test_dataset_sim itself is the original or a Subset (not an issue here)
        filename_to_print = test_dataset_sim.dataset.metadata_df.iloc[i]['Image Name'] if hasattr(test_dataset_sim, 'dataset') else test_dataset_sim.metadata_df.iloc[i]['Image Name']
        print(f"Sample {i+1} (Filename: {filename_to_print}):")
        print(f"  Target:    {[f'{x:.3f}' for x in all_targets_test[i]]}")
        print(f"  Predicted: {[f'{x:.3f}' for x in all_predictions_test[i]]}")
        sample_mae = np.mean(np.abs(all_targets_test[i] - all_predictions_test[i]))
        print(f"  Sample MAE: {sample_mae:.4f}\n")
else:
    print("\nNo test data provided for final evaluation.")

print(f"Outputs (like loss plot) saved in directory: {OUTPUT_DIR_SIM}")


# --- 6. Evaluation on Test Set (Using nn.MSELoss(), model output is already ~0-1) ---
if test_dataloader_sim:
    print("\nFinal Evaluation on Test Set (using model from end of training)...")
    simulator_model.eval() # Set model to evaluation mode
    final_test_mse_loss_sum = 0.0 # Sum of batch MSE losses * batch_size
    final_test_abs_diff_sum = 0.0 # Sum of absolute differences for MAE
    num_total_absorbance_values_in_test = 0 # Total number of individual absorbance values

    all_targets_test_list = []
    all_predictions_test_list = []

    with torch.no_grad():
        for images_test, targets_test in test_dataloader_sim:
            images_test = images_test.to(device)
            targets_test = targets_test.to(device)

            outputs_test = simulator_model(images_test) # Output is already ~0-1 due to model's Sigmoid

            # Calculate MSE loss for this batch
            loss_test = criterion_sim(outputs_test, targets_test) # criterion_sim is nn.MSELoss()
            final_test_mse_loss_sum += loss_test.item() * images_test.size(0) # Accumulate sum of (batch_loss * batch_size)

            # For MAE and plotting, outputs_test can be used directly
            final_test_abs_diff_sum += torch.sum(torch.abs(outputs_test - targets_test)).item()
            num_total_absorbance_values_in_test += targets_test.numel()

            all_targets_test_list.append(targets_test.cpu().numpy())
            all_predictions_test_list.append(outputs_test.cpu().numpy())

    # Calculate average MSE loss over the entire test dataset
    # len(test_dataset_sim.dataset) gives the total number of samples in the original test dataset
    avg_final_test_mse_loss = final_test_mse_loss_sum / len(test_dataset_sim)
    print(f"Average Final Test Loss (MSE): {avg_final_test_mse_loss:.6f}")

    # Calculate overall MAE on the test set
    avg_final_test_mae = final_test_abs_diff_sum / num_total_absorbance_values_in_test if num_total_absorbance_values_in_test > 0 else float('nan')
    print(f"Average Final Test MAE: {avg_final_test_mae:.6f}")

    # Concatenate all targets and predictions
    all_targets_test = np.concatenate(all_targets_test_list, axis=0)
    all_predictions_test = np.concatenate(all_predictions_test_list, axis=0)

    print("\nSample Test Predictions (Target vs Predicted) with Plots:")
    num_samples_to_plot = min(5, len(all_targets_test))
    angle_indices = np.arange(NUM_ANGLES) # For x-axis of the plot

    for i in range(num_samples_to_plot):
        # Accessing filename from the original dataset object
        # This assumes test_dataset_sim is the direct Dataset object
        filename_to_print = test_dataset_sim.metadata_df.iloc[i][test_dataset_sim.filename_col]

        target_values = all_targets_test[i]
        predicted_values = all_predictions_test[i] # Use directly

        print(f"\nSample {i+1} (Filename: {filename_to_print}):")
        print(f"  Target:    {[f'{x:.3f}' for x in target_values]}")
        print(f"  Predicted: {[f'{x:.3f}' for x in predicted_values]}")
        sample_mae = np.mean(np.abs(target_values - predicted_values))
        print(f"  Sample MAE: {sample_mae:.4f}")

        # Create a plot for this sample
        plt.figure(figsize=(8, 5))
        plt.plot(angle_indices, target_values, 'bo-', label='Target Absorbance', linewidth=2, markersize=5)
        plt.plot(angle_indices, predicted_values, 'ro--', label='Predicted Absorbance', linewidth=2, markersize=5)
        plt.xticks(angle_indices, [f'{j}' for j in range(NUM_ANGLES)]) # Simple angle index
        plt.xlabel("Angle Index (0-14)")
        plt.ylabel("Absorbance")
        plt.title(f"Absorbance vs. Angle Index - Sample {i+1}\nFile: {filename_to_print}\nMAE: {sample_mae:.4f}")
        plt.legend()
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.ylim(-0.1, 1.1) # y-axis limits for absorbance
        plot_filename = f"test_sample_{i+1}_prediction_vs_target.png"
        plt.savefig(os.path.join(OUTPUT_DIR_SIM, plot_filename))
        plt.show()
        print(f"Saved plot: {os.path.join(OUTPUT_DIR_SIM, plot_filename)}")
else:
    print("\nNo test data provided for final evaluation.")

print(f"\nOutputs (like loss plot and sample plots) saved in directory: {OUTPUT_DIR_SIM}")