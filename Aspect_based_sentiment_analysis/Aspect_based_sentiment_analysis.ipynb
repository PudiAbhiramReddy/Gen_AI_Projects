{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_jTeAIrrXdg",
    "outputId": "94ab621d-6bb5-4a6c-f03a-fa5f01e5a6c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# @title Step 0: Setup and Installations\n",
    "!pip install transformers datasets seqeval evaluate accelerate -q -U # Added accelerate for Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ztcb5WksBvJ"
   },
   "outputs": [],
   "source": [
    "# @title Step 0: Setup and Installations\n",
    "\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "from datasets import Dataset as HfDataset, DatasetDict # Renamed Hugging Face Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support # For LSTM eval\n",
    "from collections import Counter, defaultdict\n",
    "import evaluate # Use the evaluate library for metrics\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgB2rhFqszUL"
   },
   "outputs": [],
   "source": [
    "# --- Download NLTK data ---\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError: # Use LookupError instead of DownloadError\n",
    "    nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuiIvwmwsCzk"
   },
   "outputs": [],
   "source": [
    "# Suppress excessive warnings\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"datasets\").setLevel(logging.ERROR) # Reduce datasets logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BBdyNneqtMMZ",
    "outputId": "0cfd3997-8b92-4964-a6e4-69d2383967c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Polarity Labels: ['positive', 'negative', 'neutral', 'conflict']\n",
      "Polarity Label Map: {'positive': 0, 'negative': 1, 'neutral': 2, 'conflict': 3}\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "XML_FILE = 'Restaurants_Train.xml' # Make sure this file is uploaded\n",
    "TRANSFORMER_MODEL_NAME = 'roberta-base' # Name consistency\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# --- LSTM Hyperparameters ---\n",
    "LSTM_EMBEDDING_DIM = 100\n",
    "LSTM_HIDDEN_DIM = 128\n",
    "LSTM_NUM_LAYERS = 1\n",
    "LSTM_DROPOUT = 0.3\n",
    "LSTM_LR = 0.001\n",
    "LSTM_EPOCHS = 20 # Train LSTM for a bit longer\n",
    "LSTM_BATCH_SIZE = 16\n",
    "LSTM_MAX_SEQ_LEN = 100 # Max sequence length for LSTM input\n",
    "\n",
    "# --- Full Polarity Labels ---\n",
    "POLARITY_LIST = ['positive', 'negative', 'neutral', 'conflict'] # All classes\n",
    "POLARITY_MAP = {label: i for i, label in enumerate(POLARITY_LIST)}\n",
    "NUM_POLARITY_LABELS = len(POLARITY_LIST)\n",
    "print(f\"Full Polarity Labels: {POLARITY_LIST}\")\n",
    "print(f\"Polarity Label Map: {POLARITY_MAP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AGmYBaNGsO_8",
    "outputId": "8a0e59be-e28c-4630-c948-229b822b8f11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# --- Check for GPU ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPZAr4JUsTie"
   },
   "outputs": [],
   "source": [
    "# @title Step 1: Data Loading and Parsing (All Polarities)\n",
    "\n",
    "def parse_restaurant_xml_full(xml_file):\n",
    "    \"\"\"Parses the SemEval Restaurant XML format, keeping all polarities.\"\"\"\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    data = []\n",
    "\n",
    "    for sentence_elem in root.findall('.//sentence'):\n",
    "        sentence_id = sentence_elem.get('id')\n",
    "        text_elem = sentence_elem.find('text')\n",
    "        text = text_elem.text if text_elem is not None else None\n",
    "\n",
    "        if not text:\n",
    "            print(f\"Warning: Sentence {sentence_id} has no text.\")\n",
    "            continue\n",
    "\n",
    "        aspect_terms = []\n",
    "        aspect_term_elems = sentence_elem.find('aspectTerms')\n",
    "        if aspect_term_elems is not None:\n",
    "            for term_elem in aspect_term_elems.findall('aspectTerm'):\n",
    "                term_text = term_elem.get('term')\n",
    "                polarity = term_elem.get('polarity')\n",
    "                try:\n",
    "                    # Use get with default to handle missing attributes gracefully\n",
    "                    from_str = term_elem.get('from')\n",
    "                    to_str = term_elem.get('to')\n",
    "\n",
    "                    if from_str is None or to_str is None or term_text is None or polarity is None:\n",
    "                         print(f\"Skipping term due to missing attributes in sentence {sentence_id}: {term_elem.attrib}\")\n",
    "                         continue\n",
    "\n",
    "                    from_idx = int(from_str)\n",
    "                    to_idx = int(to_str)\n",
    "                    # Use POLARITY_MAP, default to -1 if polarity not found (shouldn't happen with this data)\n",
    "                    pol_label = POLARITY_MAP.get(polarity, -1)\n",
    "\n",
    "                    if pol_label != -1: # Ensure polarity is valid\n",
    "                         aspect_terms.append({\n",
    "                             'term': term_text,\n",
    "                             'polarity': polarity,\n",
    "                             'polarity_label': pol_label,\n",
    "                             'from': from_idx,\n",
    "                             'to': to_idx\n",
    "                         })\n",
    "                    else:\n",
    "                         print(f\"Warning: Unknown polarity '{polarity}' for term '{term_text}' in sentence {sentence_id}. Skipping.\")\n",
    "\n",
    "                except (ValueError, TypeError) as e:\n",
    "                     print(f\"Skipping term due to invalid from/to in sentence {sentence_id}: {term_elem.attrib} - Error: {e}\")\n",
    "\n",
    "        # We don't need aspect categories for this specific request\n",
    "        # aspect_categories = [] # ... parsing logic if needed ...\n",
    "\n",
    "        data.append({\n",
    "            'id': sentence_id,\n",
    "            'text': text,\n",
    "            'aspect_terms': aspect_terms, # List of terms with full polarity info\n",
    "            # 'aspect_categories': aspect_categories # Include if needed later\n",
    "        })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "auBJczEPsahT",
    "outputId": "2dc8e255-b282-4bae-feaf-05812c583224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3044 sentences.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Data ---\n",
    "raw_data_full = parse_restaurant_xml_full(XML_FILE)\n",
    "print(f\"Loaded {len(raw_data_full)} sentences.\")\n",
    "if not raw_data_full:\n",
    "     raise ValueError(\"Failed to load any data. Check XML file path and format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1834htgEsebf",
    "outputId": "eef645cb-e8fa-4201-ebaa-297e44b8ac25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Raw Data Entry (Full Polarity):\n",
      "{\n",
      "  \"id\": \"3121\",\n",
      "  \"text\": \"But the staff was so horrible to us.\",\n",
      "  \"aspect_terms\": [\n",
      "    {\n",
      "      \"term\": \"staff\",\n",
      "      \"polarity\": \"negative\",\n",
      "      \"polarity_label\": 1,\n",
      "      \"from\": 8,\n",
      "      \"to\": 13\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "print(\"\\nExample Raw Data Entry (Full Polarity):\")\n",
    "example_entry = next((item for item in raw_data_full if item['aspect_terms']), raw_data_full[0]) # Show first with aspects or just first\n",
    "print(json.dumps(example_entry, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pA2i420gumZ6",
    "outputId": "762b554d-467d-4ee7-b4c6-0d229642f5f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 2435, Test examples: 609\n",
      "\n",
      "HF Dataset structure (Full Polarity):\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'text', 'aspect_terms', '__index_level_0__'],\n",
      "        num_rows: 2435\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'text', 'aspect_terms', '__index_level_0__'],\n",
      "        num_rows: 609\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# --- Split Data ---\n",
    "# Keep original list format for LSTM, create HF Dataset for Transformers\n",
    "df = pd.DataFrame(raw_data_full)\n",
    "train_df, test_df = train_test_split(df, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "train_raw_list = train_df.to_dict('records') # For LSTM\n",
    "test_raw_list = test_df.to_dict('records')   # For LSTM & Final Eval Loop\n",
    "\n",
    "# --- Create Hugging Face Datasets (for Transformer models) ---\n",
    "train_dataset_hf = HfDataset.from_pandas(train_df)\n",
    "test_dataset_hf = HfDataset.from_pandas(test_df)\n",
    "raw_datasets_hf = DatasetDict({'train': train_dataset_hf, 'test': test_dataset_hf})\n",
    "\n",
    "print(f\"Train examples: {len(train_raw_list)}, Test examples: {len(test_raw_list)}\")\n",
    "print(\"\\nHF Dataset structure (Full Polarity):\")\n",
    "print(raw_datasets_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301,
     "referenced_widgets": [
      "beee9dbb0ca6494bafab018f72be92e3",
      "896813b6340e45a4a0a239a0f1ac7079",
      "5ddba2e6948d413dbd204e56aff1eb20",
      "2085e8c3742543bdaf86c17029d0f5b5",
      "77f723121f7043a099363a2802290d02",
      "77e8617aa8cc458eb6b374a167d5c86d",
      "b542601fb4394877b39b970b7b39c2a4",
      "89c2283c04da4180b5099541b6173c99",
      "8cd321662cf042c68106377b92571fcd",
      "6438886c173346eb9d4b2a830d06e6c3",
      "90ff85b0b91648ddae4e69cb1921a7b9",
      "ff4a897875614729871e145f493c8ef5",
      "d578fefc24e1482b836081703c476398",
      "cd267bce8b0d4368879b397b14f4aba6",
      "96e7ef42087e480b89e27f0570094a39",
      "c9861537b4eb451cad0071b5b86177bc",
      "68df475823374adfa32c45c21a32882d",
      "915c9586afdc4584a491bf407225e4a9",
      "803e4378583b470fb00cb36ba44582d1",
      "9da2d060940d482e8d2bd34f8c4eb0a2",
      "f4b778812cc1421f826198bd6c754ac5",
      "f1e6a5574bf34f21811c0a586ae2bc67",
      "e601a7fc94e54ab2a49d57f93239efba",
      "a3d4d533adfb41d99598a2fdfaa214b7",
      "ebc773f59282419eaa416b1b53dcea05",
      "6509ec4c9a424e078d2651cd9c68dd3a",
      "bd1a0e93b1c0487c9bc05a285d3c4de7",
      "4f15cb29782c4970861b41ca55b0bb2a",
      "36a7da39be6e479a8643d3e960e26d99",
      "0e35517bf11b41899f80d28190166d99",
      "db816ff6990f402db354f74313f00605",
      "9d6b1a8f5cd144f4932e6d322bb14ae1",
      "45fe62f5dd844f5fa75f5bfcef0bc62f",
      "7f20fc70658b4ddf8ca9f9d380c8517b",
      "5fe9f92990494966bff4a7cfd785565c",
      "dd1c3c6adafd4436a209b2246aff1b95",
      "f2a70b033e224be0be76060d3b7d882e",
      "cc6db6412a434d15b23249039a0f51a7",
      "4c2ac74fa16f41d5afd50d7f2ee5dfc0",
      "3b180e0590854247a1cf04a77b8487e0",
      "7ea8c23c29bc4ecb88e7c4b087dd6169",
      "8c38648566be4f369941e41442d8025e",
      "23ac2115175c44a3b9d4f60119202526",
      "52e480383c6c4ca69ad81fa3d593d08f",
      "ac4776d3849249c9b2e6447ff7d4e4a7",
      "29c52936fc184ef99533101e265b68cd",
      "41f7054a6cb7450881f05902cd22c227",
      "ba7e52d36a5a4435b8d9c47f39ee898c",
      "97adcdae11794f748b4308d62e015336",
      "496235692ae84f38af39aeff72299c11",
      "f95a48f299fa41129c44ac29c36a8ae2",
      "7f585ebc92fa484495f8812a07a95935",
      "b7b886547ec1470ab463a7b593212f07",
      "fbdd74fe7d864b49a8b47f1d90ef84de",
      "c71bf777c25943198f276711b50966a4"
     ]
    },
    "id": "r43uc4umssC1",
    "outputId": "4938ec36-7954-452a-db76-0cff8a65e7f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beee9dbb0ca6494bafab018f72be92e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4a897875614729871e145f493c8ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e601a7fc94e54ab2a49d57f93239efba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f20fc70658b4ddf8ca9f9d380c8517b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4776d3849249c9b2e6447ff7d4e4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Step 2a: Data Preparation for Aspect Term Extraction (ATE)\n",
    "\n",
    "# --- ATE Labels (BIO) ---\n",
    "ate_label_list = ['O', 'B-ASP', 'I-ASP']\n",
    "ate_label_map = {label: i for i, label in enumerate(ate_label_list)}\n",
    "num_ate_labels = len(ate_label_list)\n",
    "\n",
    "# --- Tokenizer ---\n",
    "# Ensure tokenizer is loaded only once if possible\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae5TpVMos0jO"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels_ate(examples):\n",
    "    \"\"\"Tokenizes text and aligns character-level spans to token-level BIO labels for ATE.\"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=False,\n",
    "        return_offsets_mapping=True,\n",
    "        max_length=512, # Set a reasonable max length\n",
    "        padding=False # Collator will handle padding\n",
    "    )\n",
    "    all_labels = []\n",
    "\n",
    "    for i, sentence_aspects in enumerate(examples[\"aspect_terms\"]): # Use 'aspect_terms' field\n",
    "        doc_labels = []\n",
    "        offset_mapping = tokenized_inputs[\"offset_mapping\"][i]\n",
    "        doc_labels = [ate_label_map['O']] * len(offset_mapping) # Initialize with 'O'\n",
    "\n",
    "        for aspect_info in sentence_aspects:\n",
    "            start_char = aspect_info['from']\n",
    "            end_char = aspect_info['to']\n",
    "\n",
    "            # Map character spans to token indices\n",
    "            token_start_index = -1\n",
    "            token_end_index = -1\n",
    "            for idx, (start, end) in enumerate(offset_mapping):\n",
    "                if start == 0 and end == 0: continue # Skip special tokens\n",
    "                if token_start_index == -1 and start <= start_char < end:\n",
    "                    token_start_index = idx\n",
    "                if start < end_char:\n",
    "                    token_end_index = idx\n",
    "\n",
    "            # Assign BIO labels\n",
    "            if token_start_index != -1 and token_end_index != -1 and token_start_index <= token_end_index:\n",
    "                doc_labels[token_start_index] = ate_label_map['B-ASP']\n",
    "                for k in range(token_start_index + 1, token_end_index + 1):\n",
    "                    # Check if k is within bounds before assigning\n",
    "                    if k < len(doc_labels):\n",
    "                         doc_labels[k] = ate_label_map['I-ASP']\n",
    "                    # else:\n",
    "                         # This case (k >= len(doc_labels)) might occur if aspect span goes beyond max_length cutoff\n",
    "                         # print(f\"Warning: Token index {k} out of bounds for doc_labels (len {len(doc_labels)}) in sentence {i}\")\n",
    "\n",
    "\n",
    "        # Align labels with word pieces, setting subsequent pieces to -100\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        if word_ids is None: # Safety check\n",
    "            all_labels.append([-100] * len(doc_labels))\n",
    "            continue\n",
    "\n",
    "        previous_word_idx = None\n",
    "        final_labels = []\n",
    "        for k, word_idx in enumerate(word_ids):\n",
    "            if k >= len(doc_labels): break # Stop if word_ids index exceeds labels list length\n",
    "\n",
    "            if word_idx is None: # Special token\n",
    "                final_labels.append(-100)\n",
    "            elif word_idx != previous_word_idx: # First token of a new word\n",
    "                final_labels.append(doc_labels[k])\n",
    "            else: # Subsequent token of the same word\n",
    "                # If part of an aspect (I-ASP), keep the label, otherwise ignore\n",
    "                if doc_labels[k] == ate_label_map['I-ASP']:\n",
    "                    final_labels.append(ate_label_map['I-ASP'])\n",
    "                else:\n",
    "                    final_labels.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(final_labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    # Remove mapping, not needed by model\n",
    "    if \"offset_mapping\" in tokenized_inputs:\n",
    "        tokenized_inputs.pop(\"offset_mapping\")\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359,
     "referenced_widgets": [
      "23e87004a18b4963b3b1b89885cd91f2",
      "c0ae944672384488ae5a485a0dcd8b4b",
      "cec6b30df4c14cb7b76ae907da26c7d2",
      "01d75ee13ddc49a790a86d4f3b41ff2b",
      "be6aa65374b04524a75c9f322b8c822c",
      "09932022609847868fa2a6c3c94da8b2",
      "0985d98349b846149452f9351abbf196",
      "42249fc76dbf462cae64d8e7934f6e54",
      "b829d5ad82214aed83d045f5281f1aca",
      "ceeb9e1aed9f444094a846194aa8b6ae",
      "e65c5a662dda415db063259f66608587",
      "d822e6e6776644559b009bd725fb6397",
      "7b91a83f1c694385bcca75ccff33efb9",
      "667d90b8956f4961b8b09beca83c29d5",
      "eccff66aa7c74ca89ca0865fd44acf09",
      "df2c93d3046c46ca95ca25d8d775fb3d",
      "594f423c13544fcaac171eec51159020",
      "62f12bb70a53439491bc4ec4e0db0cb5",
      "06dc085b24d446a18b186efed8d8cb06",
      "c1145f9f5be74d24aefbef82309a344d",
      "d92af3628ac8486ea855a65b2c067457",
      "3e0a15bfdcdb4b4a85cd0119c0eb6e65"
     ]
    },
    "id": "wVquNRCevURw",
    "outputId": "edc4de76-cfad-47c0-e848-1252f7027ee0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e87004a18b4963b3b1b89885cd91f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2435 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d822e6e6776644559b009bd725fb6397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/609 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized ATE dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2435\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 609\n",
      "    })\n",
      "})\n",
      "\n",
      "Example ATE tokenized entry (showing input_ids and labels):\n",
      "Input IDs: [0, 2387, 1623, 56, 5, 10969, 3998, 879, 6, 14140, 6, 8, 2480, 6353, 8, 37, 3776, 70, 155, 7484, 4, 2]\n",
      "Labels:    [-100, 0, 0, 0, 0, 1, 2, 2, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "# --- Apply Tokenization and Alignment for ATE ---\n",
    "cols_to_remove_ate = [col for col in raw_datasets_hf[\"train\"].column_names if col not in [\"text\", \"aspect_terms\"]]\n",
    "tokenized_datasets_ate = raw_datasets_hf.map(\n",
    "    tokenize_and_align_labels_ate, batched=True,\n",
    "    remove_columns=cols_to_remove_ate + [\"aspect_terms\"]\n",
    ")\n",
    "\n",
    "print(\"\\nTokenized ATE dataset structure:\")\n",
    "print(tokenized_datasets_ate)\n",
    "print(\"\\nExample ATE tokenized entry (showing input_ids and labels):\")\n",
    "print(f\"Input IDs: {tokenized_datasets_ate['train'][0]['input_ids']}\")\n",
    "print(f\"Labels:    {tokenized_datasets_ate['train'][0]['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "fe09189de2364541a6de36f05fc86eee",
      "21d66e626ea7450da397f31a110b6bf3",
      "2602d20cf6ab4fb98c8d699da618ea0e",
      "8112824a4ae44ba298d0061252d5f0ed",
      "9e3031c2f7e34e95a2b1c707c751101d",
      "c7af690abef1442baed7de58cd25abaf",
      "0f77cf4849a74b5dbc6bf3c2fc63c65d",
      "3f5a90557dab436ebb5f2f78939d7971",
      "daccd53b896c458fba074e81a5312608",
      "5c5a112a9ebc4f6888618ce98fbf4019",
      "68541bad151b485e89f432f7451c2a90"
     ]
    },
    "id": "-AvfZYUDv43U",
    "outputId": "7fe02318-4932-448c-c81d-47b41ed77dc5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe09189de2364541a6de36f05fc86eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# @title Step 2b: Aspect Term Extraction (ATE) - Model Training\n",
    "# --- ATE Model & Training ---\n",
    "model_ate = AutoModelForTokenClassification.from_pretrained(\n",
    "    TRANSFORMER_MODEL_NAME,\n",
    "    num_labels=num_ate_labels,\n",
    "    id2label={i: l for i, l in enumerate(ate_label_list)},\n",
    "    label2id=ate_label_map\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9dd915ce83104444986c3ece893e719c",
      "ad1a71a50ab04b65b9129e54089b0707",
      "392a4df23acf4964ad7d589c808a4fd3",
      "8473c33b03e0426fabd6235d8df1f15a",
      "ebcac01d373d4d2e8ea68ee06857249c",
      "fcd1bbc49d664e8d926caa599cb1153d",
      "ed895c9567b040fb9dc1f186f62185ac",
      "8898a87225b44a22872d0580ac465177",
      "35ef079eeadc470083ef4657c12d20fd",
      "d00bfb4abfda44dd8737ff1b63ff2e5b",
      "09aa0f2a9a324b71a2cface2e15e1bde"
     ]
    },
    "id": "Amsa6n5BtiKZ",
    "outputId": "6345f645-e581-4539-b749-f5898209be75"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd915ce83104444986c3ece893e719c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Data Collator ---\n",
    "data_collator_ate = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# --- Metrics ---\n",
    "seqeval_metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbGt6Jvvto_R"
   },
   "outputs": [],
   "source": [
    "def compute_metrics_ate(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [\n",
    "        [ate_label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [ate_label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    # Handle cases where there might be no predicted/true labels after filtering -100\n",
    "    if not any(true_labels) and not any(true_predictions):\n",
    "         print(\"Warning: No labels found for metric calculation in this batch.\")\n",
    "         # Return default values or skip calculation for this batch\n",
    "         return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"accuracy\": 0.0}\n",
    "\n",
    "    results = seqeval_metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7O8uFzRJt1eo"
   },
   "outputs": [],
   "source": [
    "# --- Training Arguments ---\n",
    "training_args_ate = TrainingArguments(\n",
    "    output_dir=\"./results/ate_full\", # Distinct dir\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10, # Maybe slightly more epochs for potentially more complex patterns\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    logging_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "TiqMsfwIt8Eb",
    "outputId": "b500ffb9-df24-4ab4-e305-52549df036c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-1a5aaa024326>:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_ate = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting ATE Model Training (Full Polarity Data) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3050' max='3050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3050/3050 08:25, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.077900</td>\n",
       "      <td>0.073382</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.885790</td>\n",
       "      <td>0.868490</td>\n",
       "      <td>0.976523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.071598</td>\n",
       "      <td>0.856959</td>\n",
       "      <td>0.883134</td>\n",
       "      <td>0.869850</td>\n",
       "      <td>0.977764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.086271</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.903054</td>\n",
       "      <td>0.875724</td>\n",
       "      <td>0.977867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.101459</td>\n",
       "      <td>0.880105</td>\n",
       "      <td>0.887118</td>\n",
       "      <td>0.883598</td>\n",
       "      <td>0.979522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.116040</td>\n",
       "      <td>0.879690</td>\n",
       "      <td>0.903054</td>\n",
       "      <td>0.891219</td>\n",
       "      <td>0.980350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.133163</td>\n",
       "      <td>0.868047</td>\n",
       "      <td>0.891102</td>\n",
       "      <td>0.879423</td>\n",
       "      <td>0.977971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.134482</td>\n",
       "      <td>0.870013</td>\n",
       "      <td>0.897742</td>\n",
       "      <td>0.883660</td>\n",
       "      <td>0.978591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.146435</td>\n",
       "      <td>0.865729</td>\n",
       "      <td>0.899070</td>\n",
       "      <td>0.882085</td>\n",
       "      <td>0.978488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.151517</td>\n",
       "      <td>0.873548</td>\n",
       "      <td>0.899070</td>\n",
       "      <td>0.886126</td>\n",
       "      <td>0.979005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.152287</td>\n",
       "      <td>0.878080</td>\n",
       "      <td>0.899070</td>\n",
       "      <td>0.888451</td>\n",
       "      <td>0.979626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ATE Model Training Finished ---\n"
     ]
    }
   ],
   "source": [
    "# --- Trainer ---\n",
    "trainer_ate = Trainer(\n",
    "    model=model_ate,\n",
    "    args=training_args_ate,\n",
    "    train_dataset=tokenized_datasets_ate[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_ate[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator_ate,\n",
    "    compute_metrics=compute_metrics_ate,\n",
    ")\n",
    "\n",
    "# --- Train ---\n",
    "print(\"\\n--- Starting ATE Model Training (Full Polarity Data) ---\")\n",
    "trainer_ate.train()\n",
    "print(\"\\n--- ATE Model Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "id": "b_5lRROiuA9l",
    "outputId": "f9816c2d-5dae-4d93-f6e9-ea7f31eb1f02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating ATE Model ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [77/77 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11603996157646179, 'eval_precision': 0.8796895213454075, 'eval_recall': 0.9030544488711819, 'eval_f1': 0.891218872870249, 'eval_accuracy': 0.9803495707932568, 'eval_runtime': 1.6339, 'eval_samples_per_second': 372.733, 'eval_steps_per_second': 47.127, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate ---\n",
    "print(\"\\n--- Evaluating ATE Model ---\")\n",
    "eval_results_ate = trainer_ate.evaluate()\n",
    "print(eval_results_ate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dZiOW5suIdc",
    "outputId": "0bc02ccf-04ae-4c22-826a-82854869e924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE model saved to ./fine_tuned_ate_model_full\n"
     ]
    }
   ],
   "source": [
    "# --- Save Model ---\n",
    "ate_model_path_full = \"./fine_tuned_ate_model_full\"\n",
    "trainer_ate.save_model(ate_model_path_full)\n",
    "tokenizer.save_pretrained(ate_model_path_full) # Save tokenizer with the model\n",
    "print(f\"ATE model saved to {ate_model_path_full}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bN30-qeJuJYW"
   },
   "outputs": [],
   "source": [
    "# @title Step 2c: Aspect Term Extraction (ATE) - Inference Function (Same as before)\n",
    "\n",
    "# This function remains the same as it only extracts based on BIO tags\n",
    "def extract_aspects_from_text(text, model_path, tokenizer_path):\n",
    "    \"\"\"Uses the fine-tuned ATE model to extract aspect terms from text.\"\"\"\n",
    "    try:\n",
    "        local_tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "        local_model = AutoModelForTokenClassification.from_pretrained(model_path).to(device)\n",
    "        local_model.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading ATE model/tokenizer from {model_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    inputs = local_tokenizer(text, return_tensors=\"pt\", truncation=True, return_offsets_mapping=True, max_length=512)\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\").squeeze().tolist()\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = local_model(**inputs).logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=2).squeeze().tolist()\n",
    "    input_ids = inputs[\"input_ids\"].squeeze().tolist()\n",
    "    id2label = local_model.config.id2label\n",
    "\n",
    "    aspects = []\n",
    "    current_aspect_tokens = []\n",
    "    current_aspect_start_char = -1\n",
    "    current_aspect_end_char = -1 # Track end character precisely\n",
    "\n",
    "    for i, pred_id in enumerate(predictions):\n",
    "        if input_ids[i] in [local_tokenizer.cls_token_id, local_tokenizer.sep_token_id, local_tokenizer.pad_token_id]:\n",
    "            continue\n",
    "        start_char, end_char = offset_mapping[i]\n",
    "        if start_char == end_char: continue\n",
    "\n",
    "        pred_label = id2label[pred_id]\n",
    "\n",
    "        if pred_label == 'B-ASP':\n",
    "            if current_aspect_tokens: # Finalize previous aspect\n",
    "                final_text = text[current_aspect_start_char:current_aspect_end_char]\n",
    "                aspects.append({\"term\": final_text.strip(), \"from\": current_aspect_start_char, \"to\": current_aspect_end_char})\n",
    "            # Start new aspect\n",
    "            current_aspect_tokens = [input_ids[i]]\n",
    "            current_aspect_start_char = start_char\n",
    "            current_aspect_end_char = end_char\n",
    "        elif pred_label == 'I-ASP':\n",
    "            if current_aspect_tokens: # Continue current aspect\n",
    "                current_aspect_tokens.append(input_ids[i])\n",
    "                current_aspect_end_char = end_char # Update end char\n",
    "            # else: Ignore I-ASP without B-ASP\n",
    "        elif pred_label == 'O':\n",
    "            if current_aspect_tokens: # Finalize aspect\n",
    "                final_text = text[current_aspect_start_char:current_aspect_end_char]\n",
    "                aspects.append({\"term\": final_text.strip(), \"from\": current_aspect_start_char, \"to\": current_aspect_end_char})\n",
    "                current_aspect_tokens = []\n",
    "                current_aspect_start_char = -1\n",
    "\n",
    "    # Add last aspect if sentence ended with one\n",
    "    if current_aspect_tokens:\n",
    "        final_text = text[current_aspect_start_char:current_aspect_end_char]\n",
    "        aspects.append({\"term\": final_text.strip(), \"from\": current_aspect_start_char, \"to\": current_aspect_end_char})\n",
    "\n",
    "    return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jvuduyf5uUmy"
   },
   "outputs": [],
   "source": [
    "# @title Step 3a: Data Preparation for Aspect Sentiment Classification (ASC - Full Polarity)\n",
    "\n",
    "def prepare_asc_data_transformer(examples):\n",
    "    \"\"\"Prepares data for ASC: pairs (sentence, aspect_term) -> full_polarity_label.\"\"\"\n",
    "    processed_texts = []\n",
    "    processed_labels = []\n",
    "\n",
    "    for i in range(len(examples[\"text\"])):\n",
    "        sentence = examples[\"text\"][i]\n",
    "        aspect_terms = examples[\"aspect_terms\"][i] # Use the field with full data\n",
    "\n",
    "        for term_info in aspect_terms:\n",
    "            # Use ground truth terms and their full polarity labels for training\n",
    "            text_pair = f\"{sentence} [SEP] {term_info['term']}\"\n",
    "            processed_texts.append(text_pair)\n",
    "            # Append the polarity_label (0, 1, 2, or 3)\n",
    "            processed_labels.append(term_info['polarity_label'])\n",
    "\n",
    "    # Tokenize the pairs\n",
    "    tokenized = tokenizer(processed_texts, truncation=True, padding=False, max_length=512) # Padding handled by collator\n",
    "    tokenized['labels'] = processed_labels\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413,
     "referenced_widgets": [
      "3b69db4c5ebb45d28db5e6fecdf00fc5",
      "e035a04814a440d49b83afc00fa9033b",
      "242429b024d94962bdc333555bcef8ce",
      "23c239a36a2e40af9edca526be366a8a",
      "bc7ef8bf701e411d939016e2e32733e2",
      "26b4159c26b845aa9c9712b3a042f5bc",
      "514ae26bbd134cc4809ed3751c2795a7",
      "e5395ae3874749c59ab0388f5913fe03",
      "7e8f5decb41548949612ac062eeb4a2c",
      "3812b201df4e4f2ea4ce94752656b55b",
      "ed310c4e260140edb74b7bbf67f0551f",
      "f80cf2c185674fb5a49c40b18b6c68ed",
      "9aaa10ab07814817a5438b3c9b63a2f7",
      "3d643ee50b01431a82aafb3bc5a759be",
      "4dfa161c68c146eea1209560ee58ed55",
      "a34f135e3c4a4fe6ba10b5385d2862b9",
      "1f62a9df76364628894edf0c1288bc22",
      "2e4730bf9c3342ee86248b73322f3ac1",
      "121ac694f63d4e01b08e2216f87b33b7",
      "37b230824ffd4051850b5bf166e9cca3",
      "ca71ae6fda7344e494ebf1755ae9c818",
      "3b97b9f74e75400198289cf14f90dbba"
     ]
    },
    "id": "FVmKsR2xuYNT",
    "outputId": "1d85ac79-ddfe-481f-9bc6-5bfe004a3240"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b69db4c5ebb45d28db5e6fecdf00fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2435 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80cf2c185674fb5a49c40b18b6c68ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/609 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ASC training examples: 2946\n",
      "Number of ASC test examples: 753\n",
      "\n",
      "Tokenized ASC dataset structure (Full Polarity):\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2946\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 753\n",
      "    })\n",
      "})\n",
      "\n",
      "Example ASC tokenized entry (showing input_ids and label):\n",
      "Input IDs: [0, 2387, 1623, 56, 5, 10969, 3998, 879, 6, 14140, 6, 8, 2480, 6353, 8, 37, 3776, 70, 155, 7484, 4, 646, 3388, 510, 742, 10969, 3998, 879, 2]\n",
      "Label:     0 (positive)\n"
     ]
    }
   ],
   "source": [
    "# --- Apply ASC Data Preparation ---\n",
    "# Use the original raw_datasets which contains the ground truth terms and full polarities\n",
    "tokenized_datasets_asc_transformer = raw_datasets_hf.map(\n",
    "    prepare_asc_data_transformer, batched=True,\n",
    "    remove_columns=raw_datasets_hf[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# Filter out examples where processing might have failed (e.g., no terms)\n",
    "# The map function handles batching, so filtering might be complex here.\n",
    "# Instead, ensure the prepare function handles empty aspect lists gracefully.\n",
    "# Let's check the dataset size.\n",
    "print(f\"Number of ASC training examples: {len(tokenized_datasets_asc_transformer['train'])}\")\n",
    "print(f\"Number of ASC test examples: {len(tokenized_datasets_asc_transformer['test'])}\")\n",
    "if len(tokenized_datasets_asc_transformer['train']) == 0:\n",
    "     raise ValueError(\"ASC training dataset is empty after preparation. Check data processing.\")\n",
    "\n",
    "\n",
    "print(\"\\nTokenized ASC dataset structure (Full Polarity):\")\n",
    "print(tokenized_datasets_asc_transformer)\n",
    "print(\"\\nExample ASC tokenized entry (showing input_ids and label):\")\n",
    "print(f\"Input IDs: {tokenized_datasets_asc_transformer['train'][0]['input_ids']}\")\n",
    "print(f\"Label:     {tokenized_datasets_asc_transformer['train'][0]['labels']} ({POLARITY_LIST[tokenized_datasets_asc_transformer['train'][0]['labels']]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "90a304b9de3048e68442d8dbde43438e",
      "eda4bb3c11db4ca59fd4e911ba7ac232",
      "8a12a685adf34305ae13454537d658aa",
      "b8f2a193eb184f23bff903cf6f6c1f51",
      "3a6e46729817488b9fae94d4c0e7178b",
      "72a5ea4fe08f43f182fe6660720697db",
      "d3137dc6f86a45e0a0c876f849d71fc5",
      "2c06581e97d9471691a9f5643db5d367",
      "5f708084698644e5a0507de45c53cce1",
      "35c8734bebfb4d35af3ccf558913bfdc",
      "db0f0a58a7b1435ebf518fd595f5ee78",
      "eb13a9705a484096a62085811fa05e66",
      "af3dbbcdac8040fdb577517841f42176",
      "e4704a972d2e464a999b1dc0e7898ab6",
      "e309d16dfeb14f848363e5c65edb4b99",
      "7aadfca7b23e45d1b7ced1b01a79c334",
      "3aa89f2d0a024c959f44796891c9c0e7",
      "1b465b1e09724ec78c97852d83a380f2",
      "c4787fd1bcaa49c8943de0d6f8853ad9",
      "a52878acfe9845b0ad7dcb4e5e10fc0c",
      "c0aa3f07bfba4766b4c7caa02b87e38e",
      "3c437ae672f64a498de1cccbc7bf4ed4"
     ]
    },
    "id": "GsXxcI81uiX2",
    "outputId": "743ee1b3-02d3-4e31-9434-18d366615737"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a304b9de3048e68442d8dbde43438e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb13a9705a484096a62085811fa05e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Step 3b: Aspect Sentiment Classification (ASC) - Model Training (Full Polarity)\n",
    "\n",
    "# --- Model ---\n",
    "asc_id2label = {i: label for i, label in enumerate(POLARITY_LIST)}\n",
    "asc_label2id = {label: i for i, label in enumerate(POLARITY_LIST)}\n",
    "\n",
    "model_asc = AutoModelForSequenceClassification.from_pretrained(\n",
    "    TRANSFORMER_MODEL_NAME,\n",
    "    num_labels=NUM_POLARITY_LABELS, # Should be 4\n",
    "    id2label=asc_id2label,\n",
    "    label2id=asc_label2id\n",
    ").to(device)\n",
    "\n",
    "# --- Data Collator ---\n",
    "data_collator_asc = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# --- Metrics ---\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmpzXu5Yuxyn",
    "outputId": "509a251c-78f7-453f-b98a-d4a5a82ce704"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-24e4a2661971>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_asc = Trainer(\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics_asc(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    # Use 'weighted' f1 for multi-class imbalance, 'micro' for overall accuracy equivalent, 'macro' for unweighted average\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    return {\n",
    "        \"accuracy\": acc[\"accuracy\"],\n",
    "        \"f1_weighted\": f1[\"f1\"],\n",
    "    }\n",
    "\n",
    "# --- Training Arguments ---\n",
    "training_args_asc = TrainingArguments(\n",
    "    output_dir=\"./results/asc_full\", # Distinct dir\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=5, # Match ATE epochs or adjust\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_weighted\", # Optimize for weighted F1\n",
    "    fp16=True,\n",
    "    save_total_limit=1,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "# --- Trainer ---\n",
    "trainer_asc = Trainer(\n",
    "    model=model_asc,\n",
    "    args=training_args_asc,\n",
    "    train_dataset=tokenized_datasets_asc_transformer[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_asc_transformer[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator_asc,\n",
    "    compute_metrics=compute_metrics_asc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "0vO_ewBIu8Jc",
    "outputId": "41129d6d-10f8-4ef8-8838-b11300ef95e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting ASC Model Training (Full Polarity) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1840' max='1840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1840/1840 05:42, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.739400</td>\n",
       "      <td>0.701312</td>\n",
       "      <td>0.719788</td>\n",
       "      <td>0.697246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.494800</td>\n",
       "      <td>0.584557</td>\n",
       "      <td>0.803453</td>\n",
       "      <td>0.780991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.443400</td>\n",
       "      <td>0.640456</td>\n",
       "      <td>0.816733</td>\n",
       "      <td>0.802140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.162800</td>\n",
       "      <td>0.918217</td>\n",
       "      <td>0.814077</td>\n",
       "      <td>0.809660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ASC Model Training Finished ---\n"
     ]
    }
   ],
   "source": [
    "# --- Train ---\n",
    "print(\"\\n--- Starting ASC Model Training (Full Polarity) ---\")\n",
    "trainer_asc.train()\n",
    "print(\"\\n--- ASC Model Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "id": "TIB0XpM9u_iD",
    "outputId": "1fe9d586-e061-4a21-bc1b-8e8567cfc9c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating ASC Model (Full Polarity) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [95/95 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9182167053222656, 'eval_accuracy': 0.8140770252324038, 'eval_f1_weighted': 0.8096597150301473, 'eval_runtime': 1.4098, 'eval_samples_per_second': 534.12, 'eval_steps_per_second': 67.386, 'epoch': 4.987788331071913}\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate ---\n",
    "print(\"\\n--- Evaluating ASC Model (Full Polarity) ---\")\n",
    "eval_results_asc = trainer_asc.evaluate()\n",
    "print(eval_results_asc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gj32njH1vCpt",
    "outputId": "f07845d0-9593-49c5-90e5-562b1f537c28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASC model saved to ./fine_tuned_asc_model_full\n"
     ]
    }
   ],
   "source": [
    "# --- Save Model ---\n",
    "asc_model_path_full = \"./fine_tuned_asc_model_full\"\n",
    "trainer_asc.save_model(asc_model_path_full)\n",
    "tokenizer.save_pretrained(asc_model_path_full) # Save tokenizer with the model\n",
    "print(f\"ASC model saved to {asc_model_path_full}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKJQcnnpvK_i"
   },
   "outputs": [],
   "source": [
    "# @title Step 3c: Aspect Sentiment Classification (ASC) - Inference Function (Full Polarity)\n",
    "\n",
    "def classify_aspect_sentiment_transformer(sentence, aspect_term, model_path, tokenizer_path):\n",
    "    \"\"\"Classifies the sentiment (full polarity) of a given aspect term within a sentence.\"\"\"\n",
    "    try:\n",
    "        local_tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "        local_model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "        local_model.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading ASC model/tokenizer from {model_path}: {e}\")\n",
    "        return \"error_loading_model\"\n",
    "\n",
    "    # Prepare input using the [CLS] sentence [SEP] aspect_term [SEP] format\n",
    "    text_pair = f\"{sentence} [SEP] {aspect_term}\"\n",
    "    inputs = local_tokenizer(text_pair, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        logits = local_model(**inputs).logits\n",
    "\n",
    "    predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Use the model's config to map id back to label string\n",
    "    # Ensure the loaded model has the correct id2label config\n",
    "    if hasattr(local_model.config, 'id2label'):\n",
    "         predicted_label = local_model.config.id2label[predicted_class_id]\n",
    "    else:\n",
    "         # Fallback if config is missing (shouldn't happen if saved correctly)\n",
    "         predicted_label = POLARITY_LIST[predicted_class_id]\n",
    "\n",
    "\n",
    "    # Optionally return probabilities/scores\n",
    "    # probabilities = torch.softmax(logits, dim=1).squeeze().tolist()\n",
    "    # score = probabilities[predicted_class_id]\n",
    "    # return predicted_label, score\n",
    "\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gq4QNjGmvL2v",
    "outputId": "00ed108a-f1af-40ad-b5f6-f3b08005d41c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ASC Inference Example (Full Polarity) ---\n",
      "Test Sentence: 'i recommend the thai popcorn :)'\n",
      "Extracted Aspects and Predicted Sentiments:\n",
      "- Aspect: 'thai popcorn', Predicted Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "# --- Inference Example (using results from ATE) ---\n",
    "print(\"\\n--- ASC Inference Example (Full Polarity) ---\")\n",
    "# Re-run ATE inference on a test sentence to get aspects\n",
    "test_sentence_idx = 10 # Choose an index\n",
    "test_sentence = raw_datasets_hf['test'][test_sentence_idx]['text']\n",
    "print(f\"Test Sentence: '{test_sentence}'\")\n",
    "\n",
    "extracted_aspects = extract_aspects_from_text(test_sentence, ate_model_path_full, ate_model_path_full)\n",
    "\n",
    "if extracted_aspects:\n",
    "    print(\"Extracted Aspects and Predicted Sentiments:\")\n",
    "    for aspect in extracted_aspects:\n",
    "        term = aspect['term']\n",
    "        predicted_sentiment = classify_aspect_sentiment_transformer(\n",
    "            test_sentence, term, asc_model_path_full, asc_model_path_full\n",
    "        )\n",
    "        print(f\"- Aspect: '{term}', Predicted Sentiment: {predicted_sentiment}\")\n",
    "else:\n",
    "    print(\"No aspects were extracted by ATE for this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsb8aVh8vTHI"
   },
   "outputs": [],
   "source": [
    "# @title Step 4: Full Pipeline Inference Example\n",
    "\n",
    "def run_absa_pipeline_full(text, ate_model_path, asc_model_path):\n",
    "    \"\"\"Runs the ATE -> ASC pipeline for full polarity classification.\"\"\"\n",
    "    results = {\"text\": text, \"aspects\": []}\n",
    "\n",
    "    # 1. Aspect Term Extraction (ATE)\n",
    "    ate_tokenizer = AutoTokenizer.from_pretrained(ate_model_path) # Re-load locally if needed\n",
    "    extracted_terms = extract_aspects_from_text(text, ate_model_path, ate_model_path)\n",
    "\n",
    "    # 2. Aspect Sentiment Classification (ASC) for each extracted term\n",
    "    if extracted_terms:\n",
    "        # Load ASC resources once if classifying multiple terms for the same sentence\n",
    "        asc_tokenizer = AutoTokenizer.from_pretrained(asc_model_path)\n",
    "        asc_model = AutoModelForSequenceClassification.from_pretrained(asc_model_path).to(device)\n",
    "        asc_model.eval()\n",
    "\n",
    "        for term_info in extracted_terms:\n",
    "            term_text = term_info['term']\n",
    "\n",
    "            # Prepare input for ASC\n",
    "            text_pair = f\"{text} [SEP] {term_text}\"\n",
    "            inputs = asc_tokenizer(text_pair, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "\n",
    "            # Get prediction\n",
    "            with torch.no_grad():\n",
    "                logits = asc_model(**inputs).logits\n",
    "            predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "            sentiment = asc_model.config.id2label[predicted_class_id] # Use model's map\n",
    "\n",
    "            results[\"aspects\"].append({\n",
    "                \"term\": term_text,\n",
    "                \"from\": term_info[\"from\"],\n",
    "                \"to\": term_info[\"to\"],\n",
    "                \"sentiment\": sentiment\n",
    "            })\n",
    "    else:\n",
    "        print(f\"Pipeline: No aspect terms extracted for: '{text}'\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bFMc13GHva3U",
    "outputId": "2ffe8e26-d08b-4beb-c9fd-a0681f1957af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Full Pipeline Inference Examples (Full Polarity) ---\n",
      "\n",
      "Input: But the staff was so horrible to us.\n",
      "Output:\n",
      "{\n",
      "  \"text\": \"But the staff was so horrible to us.\",\n",
      "  \"aspects\": [\n",
      "    {\n",
      "      \"term\": \"staff\",\n",
      "      \"from\": 8,\n",
      "      \"to\": 13,\n",
      "      \"sentiment\": \"negative\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Input: The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not.\n",
      "Output:\n",
      "{\n",
      "  \"text\": \"The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not.\",\n",
      "  \"aspects\": [\n",
      "    {\n",
      "      \"term\": \"food\",\n",
      "      \"from\": 4,\n",
      "      \"to\": 8,\n",
      "      \"sentiment\": \"positive\"\n",
      "    },\n",
      "    {\n",
      "      \"term\": \"kitchen\",\n",
      "      \"from\": 55,\n",
      "      \"to\": 62,\n",
      "      \"sentiment\": \"positive\"\n",
      "    },\n",
      "    {\n",
      "      \"term\": \"menu\",\n",
      "      \"from\": 141,\n",
      "      \"to\": 145,\n",
      "      \"sentiment\": \"neutral\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Input: They did not have mayonnaise, forgot our toast, left out ingredients (ie cheese in an omelet), below hot temperatures and the bacon was so over cooked it crumbled on the plate when you touched it.\n",
      "Output:\n",
      "{\n",
      "  \"text\": \"They did not have mayonnaise, forgot our toast, left out ingredients (ie cheese in an omelet), below hot temperatures and the bacon was so over cooked it crumbled on the plate when you touched it.\",\n",
      "  \"aspects\": [\n",
      "    {\n",
      "      \"term\": \"mayonnaise\",\n",
      "      \"from\": 18,\n",
      "      \"to\": 28,\n",
      "      \"sentiment\": \"negative\"\n",
      "    },\n",
      "    {\n",
      "      \"term\": \"toast\",\n",
      "      \"from\": 41,\n",
      "      \"to\": 46,\n",
      "      \"sentiment\": \"negative\"\n",
      "    },\n",
      "    {\n",
      "      \"term\": \"ingredients\",\n",
      "      \"from\": 57,\n",
      "      \"to\": 68,\n",
      "      \"sentiment\": \"negative\"\n",
      "    },\n",
      "    {\n",
      "      \"term\": \"cheese\",\n",
      "      \"from\": 73,\n",
      "      \"to\": 79,\n",
      "      \"sentiment\": \"neutral\"\n",
      "    },\n",
      "    {\n",
      "      \"term\": \"omelet\",\n",
      "      \"from\": 86,\n",
      "      \"to\": 92,\n",
      "      \"sentiment\": \"neutral\"\n",
      "    },\n",
      "    {\n",
      "      \"term\": \"bacon\",\n",
      "      \"from\": 126,\n",
      "      \"to\": 131,\n",
      "      \"sentiment\": \"negative\"\n",
      "    },\n",
      "    {\n",
      "      \"term\": \"plate\",\n",
      "      \"from\": 170,\n",
      "      \"to\": 175,\n",
      "      \"sentiment\": \"neutral\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Input: It took half an hour to get our check, which was perfect since we could sit, have drinks and talk!\n",
      "Output:\n",
      "{\n",
      "  \"text\": \"It took half an hour to get our check, which was perfect since we could sit, have drinks and talk!\",\n",
      "  \"aspects\": [\n",
      "    {\n",
      "      \"term\": \"check\",\n",
      "      \"from\": 32,\n",
      "      \"to\": 37,\n",
      "      \"sentiment\": \"neutral\"\n",
      "    },\n",
      "    {\n",
      "      \"term\": \"drinks\",\n",
      "      \"from\": 82,\n",
      "      \"to\": 88,\n",
      "      \"sentiment\": \"neutral\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Run Full Pipeline ---\n",
    "print(\"\\n--- Full Pipeline Inference Examples (Full Polarity) ---\")\n",
    "\n",
    "# Example 1: Sentence ID 3121\n",
    "sentence_3121 = \"But the staff was so horrible to us.\"\n",
    "print(f\"\\nInput: {sentence_3121}\")\n",
    "result_3121 = run_absa_pipeline_full(sentence_3121, ate_model_path_full, asc_model_path_full)\n",
    "print(\"Output:\")\n",
    "print(json.dumps(result_3121, indent=2))\n",
    "\n",
    "# Example 2: Sentence ID 1634\n",
    "sentence_1634 = \"The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not.\"\n",
    "print(f\"\\nInput: {sentence_1634}\")\n",
    "result_1634 = run_absa_pipeline_full(sentence_1634, ate_model_path_full, asc_model_path_full)\n",
    "print(\"Output:\")\n",
    "print(json.dumps(result_1634, indent=2))\n",
    "\n",
    "# Example 3: Sentence ID 296 (Multiple terms, neg/neu)\n",
    "sentence_296 = \"They did not have mayonnaise, forgot our toast, left out ingredients (ie cheese in an omelet), below hot temperatures and the bacon was so over cooked it crumbled on the plate when you touched it.\"\n",
    "print(f\"\\nInput: {sentence_296}\")\n",
    "result_296 = run_absa_pipeline_full(sentence_296, ate_model_path_full, asc_model_path_full)\n",
    "print(\"Output:\")\n",
    "print(json.dumps(result_296, indent=2))\n",
    "\n",
    "# Example 4: Sentence ID 1793 (Conflict category, neutral terms)\n",
    "sentence_1793 = \"It took half an hour to get our check, which was perfect since we could sit, have drinks and talk!\"\n",
    "print(f\"\\nInput: {sentence_1793}\")\n",
    "result_1793 = run_absa_pipeline_full(sentence_1793, ate_model_path_full, asc_model_path_full)\n",
    "print(\"Output:\")\n",
    "print(json.dumps(result_1793, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ui8IrLPGo_Hy"
   },
   "source": [
    "#BaseLine Model - Using Bi Directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrXUosgzpFQp"
   },
   "outputs": [],
   "source": [
    "# --- LSTM Data Preparation ---\n",
    "def build_vocab(data, tokenizer_func=word_tokenize, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for item in data: counter.update(tokenizer_func(item['text'].lower()))\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}; idx = 2\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq: vocab[word] = idx; idx += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22ZYDvA6366v",
    "outputId": "1dcbac2a-f664-4c70-bc68-0fc152394a01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt', quiet=True) # Ensure punkt tokenizer data is downloaded\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mf1zSn8lpKv8",
    "outputId": "f24ac2fc-3565-40ca-d822-bde87f6b9e56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Vocab Size: 4063\n"
     ]
    }
   ],
   "source": [
    "lstm_vocab = build_vocab(train_raw_list)\n",
    "VOCAB_SIZE = len(lstm_vocab)\n",
    "PAD_IDX = lstm_vocab['<PAD>']\n",
    "print(f\"LSTM Vocab Size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atAXx1FnpRDe"
   },
   "outputs": [],
   "source": [
    "def find_token_span(sentence_tokens, aspect_tokens):\n",
    "    for i in range(len(sentence_tokens) - len(aspect_tokens) + 1):\n",
    "        if sentence_tokens[i:i+len(aspect_tokens)] == aspect_tokens: return i, i + len(aspect_tokens) -1\n",
    "    return -1, -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wycDljKlpct5"
   },
   "outputs": [],
   "source": [
    "class LstmAbsaDataset(TorchDataset):\n",
    "    def __init__(self, data, vocab, tokenizer_func=word_tokenize, max_len=LSTM_MAX_SEQ_LEN):\n",
    "        self.data = []; self.vocab = vocab; self.tokenizer_func = tokenizer_func\n",
    "        self.max_len = max_len; self.unk_idx = vocab.get('<UNK>', 1)\n",
    "        for item in data:\n",
    "            sentence_tokens = self.tokenizer_func(item['text'].lower())\n",
    "            for aspect_info in item['aspect_terms']:\n",
    "                aspect_tokens = self.tokenizer_func(aspect_info['term'].lower())\n",
    "                start_idx, end_idx = find_token_span(sentence_tokens, aspect_tokens)\n",
    "                if start_idx == -1: continue # Skip if aspect not found exactly\n",
    "                sentence_indices = [self.vocab.get(token, self.unk_idx) for token in sentence_tokens]\n",
    "                self.data.append({'sentence_indices': sentence_indices, 'aspect_start': start_idx,\n",
    "                                  'aspect_end': end_idx, 'label': aspect_info['polarity_label']})\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]; indices = item['sentence_indices']; seq_len = len(indices)\n",
    "        if seq_len > self.max_len:\n",
    "            indices = indices[:self.max_len]\n",
    "            item['aspect_start'] = min(item['aspect_start'], self.max_len - 1)\n",
    "            item['aspect_end'] = min(item['aspect_end'], self.max_len - 1)\n",
    "            seq_len = self.max_len\n",
    "        else: indices.extend([PAD_IDX] * (self.max_len - seq_len))\n",
    "        item['aspect_start'] = max(0, item['aspect_start'])\n",
    "        item['aspect_end'] = max(0, item['aspect_end'])\n",
    "        if item['aspect_start'] > item['aspect_end']: item['aspect_start'] = item['aspect_end']\n",
    "        return {'input_ids': torch.tensor(indices, dtype=torch.long),\n",
    "                'aspect_start': torch.tensor(item['aspect_start'], dtype=torch.long),\n",
    "                'aspect_end': torch.tensor(item['aspect_end'], dtype=torch.long),\n",
    "                'labels': torch.tensor(item['label'], dtype=torch.long)}\n",
    "\n",
    "train_dataset_lstm = LstmAbsaDataset(train_raw_list, lstm_vocab)\n",
    "test_dataset_lstm = LstmAbsaDataset(test_raw_list, lstm_vocab)\n",
    "\n",
    "if len(train_dataset_lstm) == 0: raise ValueError(\"LSTM Training dataset empty.\")\n",
    "if len(test_dataset_lstm) == 0: print(\"Warning: LSTM Test dataset empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DJo3ZH75pipp",
    "outputId": "d6de8658-c114-477e-91ab-5df150a6245d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Train Batches: 183, Test Batches: 47\n"
     ]
    }
   ],
   "source": [
    "def simple_collate_fn(batch):\n",
    "    keys = batch[0].keys(); return {k: torch.stack([item[k] for item in batch]) for k in keys}\n",
    "\n",
    "train_loader_lstm = DataLoader(train_dataset_lstm, batch_size=LSTM_BATCH_SIZE, shuffle=True, collate_fn=simple_collate_fn)\n",
    "test_loader_lstm = DataLoader(test_dataset_lstm, batch_size=LSTM_BATCH_SIZE, shuffle=False, collate_fn=simple_collate_fn) if len(test_dataset_lstm) > 0 else None\n",
    "print(f\"LSTM Train Batches: {len(train_loader_lstm)}, Test Batches: {len(test_loader_lstm) if test_loader_lstm else 0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxtAj51gpzoW"
   },
   "outputs": [],
   "source": [
    "# --- LSTM Model Definition ---\n",
    "class LstmAbsaClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                 n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, batch_first=True,\n",
    "                           dropout=dropout if n_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim) # *2 for bidirectional\n",
    "    def forward(self, input_ids, aspect_start, aspect_end):\n",
    "        embedded = self.dropout(self.embedding(input_ids))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        pooled_outputs = []\n",
    "        for i in range(outputs.shape[0]):\n",
    "            start = aspect_start[i].item(); end = min(aspect_end[i].item(), outputs.shape[1] - 1)\n",
    "            start = min(start, end)\n",
    "            pooled = torch.mean(outputs[i, start:end+1, :], dim=0) # Avg pooling\n",
    "            pooled_outputs.append(pooled)\n",
    "        pooled_batch = self.dropout(torch.stack(pooled_outputs))\n",
    "        return self.fc(pooled_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPMBKHtKqN9X",
    "outputId": "2469e28f-977c-44e3-e7f0-a3a286ea6eaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Baseline ASC (LSTM) Model ---\n",
      "Epoch 1/20 | Loss: 1.0314 | Test Acc: 0.6051 | Test F1 (w): 0.5548\n",
      "  -> New best LSTM model saved to ./baseline_lstm_model.pt\n",
      "Epoch 2/20 | Loss: 0.8683 | Test Acc: 0.6345 | Test F1 (w): 0.6042\n",
      "  -> New best LSTM model saved to ./baseline_lstm_model.pt\n",
      "Epoch 3/20 | Loss: 0.7411 | Test Acc: 0.6560 | Test F1 (w): 0.6281\n",
      "  -> New best LSTM model saved to ./baseline_lstm_model.pt\n",
      "Epoch 4/20 | Loss: 0.6316 | Test Acc: 0.6292 | Test F1 (w): 0.6275\n",
      "Epoch 5/20 | Loss: 0.5326 | Test Acc: 0.6519 | Test F1 (w): 0.6450\n",
      "  -> New best LSTM model saved to ./baseline_lstm_model.pt\n",
      "Epoch 6/20 | Loss: 0.4421 | Test Acc: 0.6439 | Test F1 (w): 0.6422\n",
      "Epoch 7/20 | Loss: 0.3722 | Test Acc: 0.6439 | Test F1 (w): 0.6519\n",
      "  -> New best LSTM model saved to ./baseline_lstm_model.pt\n",
      "Epoch 8/20 | Loss: 0.2929 | Test Acc: 0.6627 | Test F1 (w): 0.6552\n",
      "  -> New best LSTM model saved to ./baseline_lstm_model.pt\n",
      "Epoch 9/20 | Loss: 0.2439 | Test Acc: 0.6506 | Test F1 (w): 0.6485\n",
      "Epoch 10/20 | Loss: 0.2128 | Test Acc: 0.6801 | Test F1 (w): 0.6762\n",
      "  -> New best LSTM model saved to ./baseline_lstm_model.pt\n",
      "Epoch 11/20 | Loss: 0.1697 | Test Acc: 0.6760 | Test F1 (w): 0.6706\n",
      "Epoch 12/20 | Loss: 0.1570 | Test Acc: 0.6707 | Test F1 (w): 0.6661\n",
      "Epoch 13/20 | Loss: 0.1306 | Test Acc: 0.6801 | Test F1 (w): 0.6734\n",
      "Epoch 14/20 | Loss: 0.1029 | Test Acc: 0.6667 | Test F1 (w): 0.6636\n",
      "Epoch 15/20 | Loss: 0.0994 | Test Acc: 0.6760 | Test F1 (w): 0.6708\n",
      "Epoch 16/20 | Loss: 0.0986 | Test Acc: 0.6466 | Test F1 (w): 0.6506\n",
      "Epoch 17/20 | Loss: 0.0752 | Test Acc: 0.6801 | Test F1 (w): 0.6762\n",
      "  -> New best LSTM model saved to ./baseline_lstm_model.pt\n",
      "Epoch 18/20 | Loss: 0.0751 | Test Acc: 0.6881 | Test F1 (w): 0.6767\n",
      "  -> New best LSTM model saved to ./baseline_lstm_model.pt\n",
      "Epoch 19/20 | Loss: 0.0662 | Test Acc: 0.6734 | Test F1 (w): 0.6691\n",
      "Epoch 20/20 | Loss: 0.0595 | Test Acc: 0.6627 | Test F1 (w): 0.6629\n",
      "Finished LSTM Training. Best F1: 0.6767\n",
      "Loaded best LSTM model from ./baseline_lstm_model.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LstmAbsaClassifier(\n",
       "  (embedding): Embedding(4063, 100, padding_idx=0)\n",
       "  (lstm): LSTM(100, 128, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Instantiate & Train LSTM Model ---\n",
    "model_lstm = LstmAbsaClassifier(\n",
    "    VOCAB_SIZE, LSTM_EMBEDDING_DIM, LSTM_HIDDEN_DIM, NUM_POLARITY_LABELS,\n",
    "    LSTM_NUM_LAYERS, True, LSTM_DROPOUT, PAD_IDX\n",
    ").to(device)\n",
    "optimizer_lstm = optim.Adam(model_lstm.parameters(), lr=LSTM_LR)\n",
    "criterion_lstm = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\n--- Training Baseline ASC (LSTM) Model ---\")\n",
    "best_lstm_f1 = -1.0\n",
    "LSTM_MODEL_PATH = \"./baseline_lstm_model.pt\"\n",
    "\n",
    "for epoch in range(LSTM_EPOCHS):\n",
    "    model_lstm.train(); epoch_loss = 0\n",
    "    for batch in train_loader_lstm:\n",
    "        optimizer_lstm.zero_grad()\n",
    "        ids = batch['input_ids'].to(device); start = batch['aspect_start'].to(device)\n",
    "        end = batch['aspect_end'].to(device); labels = batch['labels'].to(device)\n",
    "        predictions = model_lstm(ids, start, end)\n",
    "        loss = criterion_lstm(predictions, labels); loss.backward(); optimizer_lstm.step()\n",
    "        epoch_loss += loss.item()\n",
    "    # Eval\n",
    "    model_lstm.eval(); all_preds, all_labels = [], []\n",
    "    if test_loader_lstm:\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader_lstm:\n",
    "                ids = batch['input_ids'].to(device); start = batch['aspect_start'].to(device)\n",
    "                end = batch['aspect_end'].to(device); labels = batch['labels'].to(device)\n",
    "                predictions = model_lstm(ids, start, end)\n",
    "                all_preds.extend(torch.argmax(predictions, dim=1).cpu().tolist())\n",
    "                all_labels.extend(labels.cpu().tolist())\n",
    "    epoch_avg_loss = epoch_loss / len(train_loader_lstm)\n",
    "    if test_loader_lstm and all_labels:\n",
    "        report = classification_report(all_labels, all_preds, target_names=POLARITY_LIST, zero_division=0, output_dict=True)\n",
    "        f1_w = report['weighted avg']['f1-score']; acc = report['accuracy']\n",
    "        print(f'Epoch {epoch+1}/{LSTM_EPOCHS} | Loss: {epoch_avg_loss:.4f} | Test Acc: {acc:.4f} | Test F1 (w): {f1_w:.4f}')\n",
    "        if f1_w > best_lstm_f1:\n",
    "             best_lstm_f1 = f1_w; torch.save(model_lstm.state_dict(), LSTM_MODEL_PATH)\n",
    "             print(f\"  -> New best LSTM model saved to {LSTM_MODEL_PATH}\")\n",
    "    else: print(f'Epoch {epoch+1}/{LSTM_EPOCHS} | Loss: {epoch_avg_loss:.4f} | Test Set Empty/No Preds')\n",
    "\n",
    "print(f\"Finished LSTM Training. Best F1: {best_lstm_f1:.4f}\")\n",
    "if best_lstm_f1 > -1: model_lstm.load_state_dict(torch.load(LSTM_MODEL_PATH))\n",
    "print(f\"Loaded best LSTM model from {LSTM_MODEL_PATH}\")\n",
    "model_lstm.eval() # Set to eval mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sn-Ka-diqXy3"
   },
   "outputs": [],
   "source": [
    "# --- LSTM Inference Function ---\n",
    "def classify_aspect_sentiment_lstm(sentence, aspect_term, model, vocab, tokenizer_func=word_tokenize, max_len=LSTM_MAX_SEQ_LEN):\n",
    "    model.eval(); unk_idx = vocab.get('<UNK>', 1); pad_idx = vocab.get('<PAD>', 0)\n",
    "    sentence_tokens = tokenizer_func(sentence.lower())\n",
    "    aspect_tokens = tokenizer_func(aspect_term.lower())\n",
    "    start_idx, end_idx = find_token_span(sentence_tokens, aspect_tokens)\n",
    "    if start_idx == -1: return 'neutral' # Fallback\n",
    "    indices = [vocab.get(token, unk_idx) for token in sentence_tokens]; seq_len = len(indices)\n",
    "    if seq_len > max_len:\n",
    "        indices = indices[:max_len]; start_idx = min(start_idx, max_len - 1); end_idx = min(end_idx, max_len - 1)\n",
    "    else: indices.extend([pad_idx] * (max_len - seq_len))\n",
    "    start_idx = max(0, start_idx); end_idx = max(0, end_idx)\n",
    "    if start_idx > end_idx: start_idx = end_idx\n",
    "    input_ids = torch.tensor([indices], dtype=torch.long).to(device)\n",
    "    aspect_start = torch.tensor([start_idx], dtype=torch.long).to(device)\n",
    "    aspect_end = torch.tensor([end_idx], dtype=torch.long).to(device)\n",
    "    with torch.no_grad(): predictions = model(input_ids, aspect_start, aspect_end)\n",
    "    pred_idx = torch.argmax(predictions, dim=1).item()\n",
    "    return POLARITY_LIST[pred_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJBxY04-Ayj6"
   },
   "outputs": [],
   "source": [
    "# @title Step 5: End-to-End Evaluation and Comparison (with Accuracy)\n",
    "\n",
    "def calculate_strict_metrics(true_pairs_list, pred_pairs_dict):\n",
    "    \"\"\"\n",
    "    Calculates strict P, R, F1, and Accuracy (Jaccard Index)\n",
    "    based on exact match of (term, sentiment).\n",
    "    \"\"\"\n",
    "    # Convert list of ground truth dicts to a set of (term, polarity) tuples\n",
    "    # Lowercase and strip term for robust comparison\n",
    "    true_set = set((d['term'].strip().lower(), d['polarity']) for d in true_pairs_list)\n",
    "\n",
    "    results = {}\n",
    "    # Iterate through each approach's predictions (e.g., 'pipeline', 'baseline')\n",
    "    for approach_name, pred_list in pred_pairs_dict.items():\n",
    "        # Convert list of predicted dicts to a set of (term, sentiment) tuples\n",
    "        pred_set = set((d['term'].strip().lower(), d['sentiment']) for d in pred_list)\n",
    "\n",
    "        # Calculate Intersection (True Positives)\n",
    "        true_positives = len(true_set.intersection(pred_set))\n",
    "\n",
    "        # Calculate Precision, Recall, F1\n",
    "        precision = true_positives / len(pred_set) if len(pred_set) > 0 else 0.0\n",
    "        recall = true_positives / len(true_set) if len(true_set) > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        # Calculate Union and Accuracy (Jaccard Index)\n",
    "        union_size = len(true_set) + len(pred_set) - true_positives\n",
    "        accuracy_jaccard = true_positives / union_size if union_size > 0 else 1.0 # Accuracy is 1 if both sets are empty\n",
    "\n",
    "        # Store results for this approach\n",
    "        results[approach_name] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"accuracy_jaccard\": accuracy_jaccard, # Added accuracy\n",
    "            \"tp\": true_positives,\n",
    "            \"pred_count\": len(pred_set)\n",
    "        }\n",
    "\n",
    "    results[\"true_count\"] = len(true_set)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SzxMue162Q_"
   },
   "outputs": [],
   "source": [
    " ASC_TRANSFORMER_MODEL_PATH = asc_model_path_full\n",
    " ATE_MODEL_PATH = ate_model_path_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlx3D6nIHVT4"
   },
   "source": [
    "### below cell takes more time run (10 min)!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlFWcZ4Lqnju",
    "outputId": "01579244-bc04-4e82-e5bc-213d16b8e502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running End-to-End Evaluation ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Running End-to-End Evaluation ---\")\n",
    "\n",
    "all_true_pairs_eval = [] # Store ground truth for final eval\n",
    "all_pred_pairs_eval = {'pipeline': [], 'baseline': []}\n",
    "# Load trained models explicitly for inference loop clarity\n",
    "ate_model_inf = AutoModelForTokenClassification.from_pretrained(ATE_MODEL_PATH).to(device)\n",
    "ate_tokenizer_inf = AutoTokenizer.from_pretrained(ATE_MODEL_PATH)\n",
    "\n",
    "pipeline_asc_model_inf = AutoModelForSequenceClassification.from_pretrained(ASC_TRANSFORMER_MODEL_PATH).to(device)\n",
    "pipeline_asc_tokenizer_inf = AutoTokenizer.from_pretrained(ASC_TRANSFORMER_MODEL_PATH)\n",
    "\n",
    "baseline_asc_model_inf = LstmAbsaClassifier( # Re-init architecture\n",
    "     VOCAB_SIZE, LSTM_EMBEDDING_DIM, LSTM_HIDDEN_DIM, NUM_POLARITY_LABELS,\n",
    "    LSTM_NUM_LAYERS, True, LSTM_DROPOUT, PAD_IDX\n",
    ").to(device)\n",
    "if best_lstm_f1 > -1: # Load weights if saved\n",
    "     baseline_asc_model_inf.load_state_dict(torch.load(LSTM_MODEL_PATH, map_location=device))\n",
    "baseline_asc_model_inf.eval()\n",
    "# Iterate through the original TEST data list\n",
    "for item in test_raw_list:\n",
    "    sentence = item['text']\n",
    "    sentence_id = item['id']\n",
    "\n",
    "    # --- Ground Truth ---\n",
    "    ground_truth_terms = item['aspect_terms']\n",
    "    for gt in ground_truth_terms:\n",
    "        all_true_pairs_eval.append({'sentence_id': sentence_id, 'term': gt['term'], 'polarity': gt['polarity']})\n",
    "\n",
    "    # --- Run ATE (Shared) ---\n",
    "    # Use the loaded inference versions\n",
    "    extracted_aspects = extract_aspects_from_text(sentence, ATE_MODEL_PATH, ATE_MODEL_PATH) # Path usage is okay here too\n",
    "\n",
    "    # --- Run Pipeline ASC (Transformer) ---\n",
    "    for aspect_info in extracted_aspects:\n",
    "        term = aspect_info['term']\n",
    "        if not term: continue\n",
    "        pipeline_sentiment = classify_aspect_sentiment_transformer(\n",
    "            sentence, term, ASC_TRANSFORMER_MODEL_PATH, ASC_TRANSFORMER_MODEL_PATH # Use paths\n",
    "        )\n",
    "        all_pred_pairs_eval['pipeline'].append({'sentence_id': sentence_id, 'term': term, 'sentiment': pipeline_sentiment})\n",
    "\n",
    "    # --- Run Baseline ASC (LSTM) ---\n",
    "    for aspect_info in extracted_aspects:\n",
    "        term = aspect_info['term']\n",
    "        if not term: continue\n",
    "        # Use the loaded LSTM model instance\n",
    "        baseline_sentiment = classify_aspect_sentiment_lstm(\n",
    "            sentence, term, baseline_asc_model_inf, lstm_vocab, word_tokenize, LSTM_MAX_SEQ_LEN\n",
    "        )\n",
    "        all_pred_pairs_eval['baseline'].append({'sentence_id': sentence_id, 'term': term, 'sentiment': baseline_sentiment})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFRzTS2_B1AW"
   },
   "source": [
    "#End to End Comparision between Bi Directional LSTM model(Base Line) and the TransformerModel(Distil BERT) New Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5djpK2UYqsRB",
    "outputId": "3a27eb53-30d7-4b6f-97ef-e7d91ae33578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparison Results (Test Set) ---\n",
      "\n",
      "Shared ATE Model Performance (Term Extraction Only):\n",
      "  - Precision: 0.8797\n",
      "  - Recall:    0.9031\n",
      "  - F1-Score:  0.8912\n",
      "\n",
      "End-to-End Performance (Strict Match: Term + Sentiment):\n",
      "  - Total Ground Truth Pairs: 445\n",
      "\n",
      "  Baseline (ATE + LSTM ASC):\n",
      "    - Predicted Pairs: 439\n",
      "    - Correct Pairs (TP): 288\n",
      "    - Precision: 0.6560\n",
      "    - Recall:    0.6472\n",
      "    - F1-Score:  0.6516\n",
      "\n",
      "  Pipeline (ATE + Transformer ASC):\n",
      "    - Predicted Pairs: 457\n",
      "    - Correct Pairs (TP): 334\n",
      "    - Precision: 0.7309\n",
      "    - Recall:    0.7506\n",
      "    - F1-Score:  0.7406\n"
     ]
    }
   ],
   "source": [
    "# --- Calculate Strict Metrics ---\n",
    "strict_results = calculate_strict_metrics(all_true_pairs_eval, all_pred_pairs_eval)\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\n--- Comparison Results (Test Set) ---\")\n",
    "print(f\"\\nShared ATE Model Performance (Term Extraction Only):\")\n",
    "# Use results from the trainer's evaluation directly\n",
    "print(f\"  - Precision: {eval_results_ate.get('eval_precision', 'N/A'):.4f}\")\n",
    "print(f\"  - Recall:    {eval_results_ate.get('eval_recall', 'N/A'):.4f}\")\n",
    "print(f\"  - F1-Score:  {eval_results_ate.get('eval_f1', 'N/A'):.4f}\")\n",
    "\n",
    "print(f\"\\nEnd-to-End Performance (Strict Match: Term + Sentiment):\")\n",
    "print(f\"  - Total Ground Truth Pairs: {strict_results['true_count']}\")\n",
    "\n",
    "print(\"\\n  Baseline (ATE + LSTM ASC):\")\n",
    "baseline_res = strict_results['baseline']\n",
    "print(f\"    - Predicted Pairs: {baseline_res['pred_count']}\")\n",
    "print(f\"    - Correct Pairs (TP): {baseline_res['tp']}\")\n",
    "print(f\"    - Precision: {baseline_res['precision']:.4f}\")\n",
    "print(f\"    - Recall:    {baseline_res['recall']:.4f}\")\n",
    "print(f\"    - F1-Score:  {baseline_res['f1']:.4f}\")\n",
    "#print(f\"    - Accuracy (Jaccard): {baseline_res['accuracy_jaccard']:.4f}\")\n",
    "\n",
    "print(\"\\n  Pipeline (ATE + Transformer ASC):\")\n",
    "pipeline_res = strict_results['pipeline']\n",
    "print(f\"    - Predicted Pairs: {pipeline_res['pred_count']}\")\n",
    "print(f\"    - Correct Pairs (TP): {pipeline_res['tp']}\")\n",
    "print(f\"    - Precision: {pipeline_res['precision']:.4f}\")\n",
    "print(f\"    - Recall:    {pipeline_res['recall']:.4f}\")\n",
    "print(f\"    - F1-Score:  {pipeline_res['f1']:.4f}\")\n",
    "#print(f\"    - Accuracy (Jaccard): {pipeline_res['accuracy_jaccard']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DeU45PPGQxdf",
    "outputId": "49c4b1aa-f902-49b8-9d35-19125c476cc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Sentences: Where Pipeline Corrected Baseline Error (Max 10 Unique Sentences) ---\n",
      "\n",
      "Sentence (ID 1867): The first 2 courses were very good, but the chocolate sampler was too rich for me and the dessert wine far too sweet.\n",
      "  - Ground Truth: [('courses', 'positive'), ('chocolate sampler', 'negative'), ('dessert wine', 'negative')]\n",
      "  - Pipeline Preds: [('courses', 'positive'), ('chocolate sampler', 'negative'), ('dessert wine', 'negative')]\n",
      "  - Baseline Preds: [('courses', 'positive'), ('chocolate sampler', 'negative'), ('dessert wine', 'neutral')]\n",
      "\n",
      "Sentence (ID 2479): We visited Bread Bar during January restaurant week and were so pleased with the menu selections and service.\n",
      "  - Ground Truth: [('menu selections', 'positive'), ('service', 'positive')]\n",
      "  - Pipeline Preds: [('menu selections', 'positive'), ('service', 'positive')]\n",
      "  - Baseline Preds: [('menu selections', 'positive'), ('service', 'neutral')]\n",
      "\n",
      "Sentence (ID 979): The service is descent even when this small place is packed.\n",
      "  - Ground Truth: [('service', 'positive'), ('place', 'negative')]\n",
      "  - Pipeline Preds: [('service', 'positive'), ('place', 'negative')]\n",
      "  - Baseline Preds: [('service', 'positive'), ('place', 'positive')]\n",
      "\n",
      "Sentence (ID 1660): Stuffing yourself with Japanese food is a rare thing.\n",
      "  - Ground Truth: [('Japanese food', 'neutral')]\n",
      "  - Pipeline Preds: [('Japanese food', 'neutral')]\n",
      "  - Baseline Preds: [('Japanese food', 'positive')]\n",
      "\n",
      "Sentence (ID 2646): Only wine and beer are served, but the house varities are actually quite good.\n",
      "  - Ground Truth: [('wine', 'neutral'), ('beer', 'neutral'), ('house varities', 'positive'), ('served', 'negative')]\n",
      "  - Pipeline Preds: [('wine', 'neutral'), ('beer', 'neutral'), ('served', 'neutral'), ('house varities', 'positive')]\n",
      "  - Baseline Preds: [('wine', 'positive'), ('beer', 'positive'), ('served', 'positive'), ('house varities', 'positive')]\n",
      "\n",
      "Sentence (ID 3177): We ate at this Thai place following the reviews but very unhappy with the foods.\n",
      "  - Ground Truth: [('foods', 'negative')]\n",
      "  - Pipeline Preds: [('foods', 'negative')]\n",
      "  - Baseline Preds: [('foods', 'positive')]\n",
      "\n",
      "Sentence (ID 2529): As for the bar, this is another bad idea.\n",
      "  - Ground Truth: [('bar', 'negative')]\n",
      "  - Pipeline Preds: [('bar', 'negative')]\n",
      "  - Baseline Preds: [('bar', 'neutral')]\n",
      "\n",
      "Sentence (ID 3328): Its not curry in a slurry crap, and regular run of the mill food.\n",
      "  - Ground Truth: [('curry', 'neutral'), ('food', 'neutral')]\n",
      "  - Pipeline Preds: [('curry', 'neutral'), ('food', 'neutral')]\n",
      "  - Baseline Preds: [('curry', 'negative'), ('food', 'neutral')]\n",
      "\n",
      "Sentence (ID 1011): We actually left hungry and went across the street to Wo Hop at 15 Mott street for some good chinese food.\n",
      "  - Ground Truth: [('chinese food', 'positive')]\n",
      "  - Pipeline Preds: [('chinese food', 'positive')]\n",
      "  - Baseline Preds: [('chinese food', 'neutral')]\n",
      "\n",
      "Sentence (ID 3686): Eating in, the atmosphere saves it, but at your desk, it's a very disappointing experience.\n",
      "  - Ground Truth: [('atmosphere', 'positive')]\n",
      "  - Pipeline Preds: [('atmosphere', 'positive')]\n",
      "  - Baseline Preds: [('atmosphere', 'conflict')]\n"
     ]
    }
   ],
   "source": [
    "# --- Show Specific Examples: Pipeline Correct, Baseline Incorrect (Sentence Level) ---\n",
    "print(\"\\n--- Example Sentences: Where Pipeline Corrected Baseline Error (Max 10 Unique Sentences) ---\")\n",
    "\n",
    "max_to_show = 10\n",
    "candidate_sentences_info = [] # Store info for qualifying sentences\n",
    "\n",
    "# Pre-process predictions into sets keyed by sentence_id for efficiency\n",
    "pipeline_preds_by_sent = defaultdict(set)\n",
    "for p in all_pred_pairs_eval['pipeline']:\n",
    "    pipeline_preds_by_sent[p['sentence_id']].add((p['term'].strip().lower(), p['sentiment']))\n",
    "\n",
    "baseline_preds_by_sent = defaultdict(set)\n",
    "for p in all_pred_pairs_eval['baseline']:\n",
    "    baseline_preds_by_sent[p['sentence_id']].add((p['term'].strip().lower(), p['sentiment']))\n",
    "\n",
    "\n",
    "processed_sentence_ids = set() # Avoid processing the same sentence multiple times if needed\n",
    "for item in test_raw_list:\n",
    "    sentence_id = item['id']\n",
    "    if sentence_id in processed_sentence_ids:\n",
    "         continue # Already decided if this sentence is a candidate\n",
    "\n",
    "    sentence = item['text']\n",
    "    ground_truth_aspects = item['aspect_terms']\n",
    "    found_qualifying_aspect = False\n",
    "\n",
    "    current_pipeline_preds_set = pipeline_preds_by_sent.get(sentence_id, set())\n",
    "    current_baseline_preds_set = baseline_preds_by_sent.get(sentence_id, set())\n",
    "\n",
    "    # Check each ground truth aspect in this sentence\n",
    "    for gt_aspect in ground_truth_aspects:\n",
    "        gt_term_normalized = gt_aspect['term'].strip().lower()\n",
    "        gt_polarity = gt_aspect['polarity'] # Use original polarity string\n",
    "        gt_pair = (gt_term_normalized, gt_polarity)\n",
    "\n",
    "        pipeline_correct = gt_pair in current_pipeline_preds_set\n",
    "        baseline_correct = gt_pair in current_baseline_preds_set\n",
    "\n",
    "        # If Pipeline got it right AND Baseline got it wrong for *at least one* aspect\n",
    "        if pipeline_correct and not baseline_correct:\n",
    "            found_qualifying_aspect = True\n",
    "            break # Found one, no need to check other aspects in this sentence\n",
    "\n",
    "    # If this sentence qualifies, store its info\n",
    "    if found_qualifying_aspect:\n",
    "        # Store all predictions for this sentence for later display\n",
    "        pipeline_preds_for_sent = [(p['term'], p['sentiment']) for p in all_pred_pairs_eval['pipeline'] if p['sentence_id'] == sentence_id]\n",
    "        baseline_preds_for_sent = [(p['term'], p['sentiment']) for p in all_pred_pairs_eval['baseline'] if p['sentence_id'] == sentence_id]\n",
    "\n",
    "        candidate_sentences_info.append({\n",
    "            'id': sentence_id,\n",
    "            'text': sentence,\n",
    "            'ground_truth': [(gt['term'], gt['polarity']) for gt in ground_truth_aspects],\n",
    "            'pipeline_preds': pipeline_preds_for_sent,\n",
    "            'baseline_preds': baseline_preds_for_sent\n",
    "        })\n",
    "\n",
    "    processed_sentence_ids.add(sentence_id) # Mark as processed\n",
    "\n",
    "# --- Print Selected Examples ---\n",
    "import random\n",
    "random.shuffle(candidate_sentences_info) # Shuffle to get a random sample\n",
    "\n",
    "count_shown = 0\n",
    "for example_info in candidate_sentences_info:\n",
    "    if count_shown >= max_to_show:\n",
    "        break\n",
    "\n",
    "    print(f\"\\nSentence (ID {example_info['id']}): {example_info['text']}\")\n",
    "    print(f\"  - Ground Truth: {example_info['ground_truth'] if example_info['ground_truth'] else '[]'}\")\n",
    "    print(f\"  - Pipeline Preds: {example_info['pipeline_preds'] if example_info['pipeline_preds'] else '[]'}\")\n",
    "    print(f\"  - Baseline Preds: {example_info['baseline_preds'] if example_info['baseline_preds'] else '[]'}\")\n",
    "    count_shown += 1\n",
    "\n",
    "if count_shown == 0:\n",
    "    print(\"\\nNo sentence examples found where Pipeline was correct for an aspect and Baseline was incorrect for the same aspect.\")\n",
    "elif count_shown < max_to_show:\n",
    "     print(f\"\\n(Showing {count_shown} unique sentences meeting the criteria)\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
